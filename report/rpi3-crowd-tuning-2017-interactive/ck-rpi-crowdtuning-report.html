<p><i>Developing efficient software and hardware has never been harder whether it is
for a tiny IoT device or an Exascale supercomputer.
Apart from the ever growing design and optimization complexity, there exist even
more fundamental problems such as lack of interdisciplinary knowledge required
for effective software/hardware co-design, and a growing technology transfer
gap between academia and industry.
<p>We introduce our new educational initiative to tackle these problems by
developing Collective Knowledge (CK), a unified experimental framework for
computer systems research and development.
We use CK to teach the community how to make their research artifacts and
experimental workflows portable, reproducible, customizable and reusable while
enabling sustainable R&D and facilitating technology transfer.
We also demonstrate how to redesign multi-objective autotuning and machine learning
as a portable and extensible CK workflow.
Such workflows enable researchers to experiment with different applications,
data sets and tools; crowdsource experimentation across diverse platforms;
share experimental results, models, visualizations; gradually expose more
design and optimization choices using a simple JSON API; and ultimately build
upon each other's findings.
<p>As the first practical step, we have implemented customizable 
compiler autotuning, crowdsourced optimization 
of diverse workloads across Raspberry Pi 3 devices,
reduced the execution time and code size by up to 40&#37;,
and applied machine learning to predict optimizations.
We hope such approach will help teach students how to build 
upon each others' work to enable efficient and self-optimizing
software/hardware/model stack for emerging workloads.
</i>
<p>


<a name="introduction">

<!-- CK_CUR_SECTION={1} -->

<h2>1&nbsp;&nbsp;Introduction</h2>
 <!-- CK_LABEL={introduction} -->

 
Many recent international roadmaps for computer systems research
appeal to reinvent computing&nbsp;[<a href="#ref_1">1</a>, <a href="#ref_2">2</a>, <a href="#ref_3">3</a>].
Indeed, developing, benchmarking, optimizing and co-designing hardware and software
has never been harder, no matter if it is for embedded and IoT devices,
or data centers and Exascale supercomputers.
This is caused by both physical limitations of existing technologies
and an unmanageable complexity of continuously changing computer systems
which already have too many design and optimization choices and objectives
to consider at all software and hardware levels&nbsp;[<a href="#ref_4">4</a>],
as conceptually shown in Figure&nbsp;<a href="#fig_introduction">1</a>.
That is why most of these roadmaps now agree with our vision
that such problems should be solved in a close collaboration
between industry, academia and end-users&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_6">6</a>].
<p>      
<p>   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_introduction"><b>Figure 1</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/d6118462b94f89da-cropped.png" width="600"><br>
<p>     <br><i>
       Too many design and optimization choices at all levels of the continuously changing software and hardware stack
       make it extremely challenging and time consuming to design efficient computer systems for realistic workloads.
     </i><br><br>

<p>     <!-- CK_LABEL={fig_introduction} -->


   </div></center>

   <p>However, after we initiated artifact evaluation (AE)&nbsp;[<a href="#ref_7">7</a>, <a href="#ref_8">8</a>]
at several premier ACM and IEEE conferences to reproduce and validate experimental results 
from published papers, we noticed an even more
fundamental problem: a growing technology transfer gap between academic
research and industrial development.
After evaluating more than 100 artifacts from the leading computer systems conferences
in the past 4 years, we noticed that only a small fraction of research artifacts 
could be easily customized, ported to other environments and hardware, reused, 
and built upon.
We have grown to believe that is this due to a lack of a common workflow framework
that could simplify implementation and sharing of artifacts and workflows as
portable, customizable and reusable components with some common API and meta information
vital for open science&nbsp;[<a href="#ref_9">9</a>].
<p>At the same time, companies are always under pressure and rarely have time to
dig into numerous academic artifacts shared as CSV/Excel files
and ``black box'' VM and Docker images, or adapt numerous ad-hoc scripts 
to realistic and ever changing workloads, software and hardware.
That is why promising techniques may remain in academia for decades while just
being incrementally improved, put on the shelf when leading students graduate,
and ``reinvented'' from time to time.
<p>Autotuning is one such example: this very popular technique has been actively
researched since the 1990s to automatically explore large optimization spaces
and improve efficiency of computer systems&nbsp;[<a href="#ref_10">10</a>, <a href="#ref_11">11</a>, <a href="#ref_12">12</a>, <a href="#ref_13">13</a>, <a href="#ref_14">14</a>, <a href="#ref_15">15</a>, <a href="#ref_16">16</a>, <a href="#ref_17">17</a>, <a href="#ref_18">18</a>, <a href="#ref_19">19</a>, <a href="#ref_20">20</a>, <a href="#ref_21">21</a>, <a href="#ref_22">22</a>, <a href="#ref_23">23</a>, <a href="#ref_24">24</a>, <a href="#ref_25">25</a>, <a href="#ref_26">26</a>, <a href="#ref_27">27</a>, <a href="#ref_28">28</a>, <a href="#ref_29">29</a>, <a href="#ref_30">30</a>, <a href="#ref_31">31</a>, <a href="#ref_32">32</a>, <a href="#ref_33">33</a>].
Every year, dozens of autotuning papers get published to optimize some components
of computer systems, improve and speed up exploration and co-design strategies,
and enable run-time adaptation.
Yet, when trying to make autotuning practical (in particular, by applying
machine learning) we faced numerous challenges with integrating such published
techniques into real, complex and continuously evolving software and hardware
stack&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_4">4</a>, <a href="#ref_6">6</a>, <a href="#ref_9">9</a>].
<p>Eventually, these problems motivated us to develop a common experimental
framework and methodology similar to physics and other natural sciences to
collaboratively improve autotuning and other techniques.
As part of this educational initiative, we implemented an extensible, portable
and technology-agnostic workflow for autotuning using the open-source
Collective Knowledge framework (CK)&nbsp;[<a href="#ref_34">34</a>, <a href="#ref_35">35</a>].
Such workflows help researchers to reuse already shared applications, kernels,
data sets and tools, or add their own ones using a common JSON API and
meta-description&nbsp;[<a href="#ref_36">36</a>]. 
Moreover, such workflows can automatically
adapt compilation and execution to a given environment on a given device
using integrated cross-platform package manager.
<p>Our approach takes advantage of a powerful and holistic top-down methodology
successfully used in physics and other sciences when learning complex systems.
The key idea is to let novice researchers first master simple compiler flag
autotuning scenarios while learning interdisciplinary techniques including
machine learning and statistical analysis.
Researchers can then gradually increase complexity to enable automatic and
collaborative co-design of the whole SW/HW stack by exposing more design and
optimization choices, multiple optimization objectives (execution time, code
size, power consumption, memory usage, platform cost, accuracy, etc.),
crowdsource autotuning across diverse devices provided by volunteers similar to
SETI@home&nbsp;[<a href="#ref_37">37</a>], 
continuously exchange and discuss optimization results, and
eventually build upon each other's results.
<p>We use our approach to optimize diverse kernels and real workloads such as <code>
zlib</code> in terms of speed and code size by crowdsourcing compiler flag autotuning
across Raspberry Pi3 devices using the default GCC 4.9.2 and the latest GCC
7.1.0 compilers.
We have been able to achieve up to 50&#37; reductions in code size and from 15&#37;
to 8 times speed ups across different workloads over the ``-O3'' baseline.
Our CK workflow and all related artifacts are available at GitHub 
to allow researchers to compare and improve various exploration strategies 
(particularly based on machine learning algorithms such as KNN, GA, SVM,
deep learning, though further documentation of APIs is still 
required)&nbsp;[<a href="#ref_24">24</a>, <a href="#ref_6">6</a>].
We have also shared all experimental results in our open repository of
optimization knowledge&nbsp;[<a href="#ref_38">38</a>, <a href="#ref_39">39</a>] to be
validated and reproduced by the community.
<p>We hope that our approach will serve as a practical foundation
for open, reproducible and sustainable computer systems research
by connecting students, scientists, end-users, hardware designers
and software developers to learn together how to co-design
the next generation of efficient and self-optimizing computer systems,
particularly via reproducible competitions such as ReQuEST&nbsp;[<a href="#ref_40">40</a>].
<p>This technical report is organized as follows.
Section&nbsp;<a href="#sec_converting">2</a> introduces the Collective Knowledge framework (CK)
and the concept of sharing artifacts as portable, customizable and reusable components.
Section&nbsp;<a href="#sec_autotuning">3</a> describes how to implement a customizable,
multi-dimensional and multi-objective autotuning as a CK workflow.
Section&nbsp;<a href="#sec_flag_autotuning">4</a> shows how to optimize compiler flags using
our universal CK autotuner.
Section&nbsp;<a href="#sec_crowdtuning">5</a> presents a snapshot of the latest optimization
results from collaborative tuning of GCC flags for numerous shared workloads
across Raspberry Pi3 devices.
Section&nbsp;<a href="#sec_collaborative">6</a> shows optimization results of zlib and other
realistic workloads for GCC 4.9.2 and GCC 7.1.0 across Raspberry Pi3 devices.
Section&nbsp;<a href="#sec_crowdfuzzing">7</a> describes how implement and crowdsource fuzzing
of compilers and systems for various bugs using our customizable CK autotuning workflow.
Section&nbsp;<a href="#sec_crowdmodeling">8</a> shows how to predict optimizations via CK for previously
unseen programs using machine learning.
Section&nbsp;<a href="#sec_features">9</a> demonstrates how to select and autotune models
and features to improve optimization predictions while reducing complexity.
Section&nbsp;<a href="#sec_datasets">10</a> shows how to enable efficient, input-aware and adaptive 
libraries and programs via CK.
Section&nbsp;<a href="#sec_competitions">11</a> presents CK as an open platform to 
support reproducible and Pareto-efficient co-design competitions 
of the whole software/hardware/model stack for emerging workloads 
such as deep learning and quantum computing.
We present future work in Section&nbsp;<a href="#sec_conclusions">12</a>.
We also included Artifact Appendix to allow students try our framework, 
participate in collaborative autotuning, gradually document APIs and
improve experimental workflows.
<p>



<a name="sec_converting">

<!-- CK_CUR_SECTION={2} -->

<h2>2&nbsp;&nbsp;Converting ad-hoc artifacts to portable and reusable components with JSON API</h2>

<!-- CK_LABEL={sec_converting} -->



Artifact sharing and reproducible experimentation are key
for our collaborative approach to machine-learning based optimization
and co-design of computer systems, which was first prototyped
during the EU-funded MILEPOST project&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_41">41</a>, <a href="#ref_24">24</a>].
Indeed, it is difficult, if not impossible, and time consuming to build useful predictive models
without large and diverse training sets (programs, data sets),
and without crowdsourcing design and optimization space exploration
across diverse hardware&nbsp;[<a href="#ref_9">9</a>, <a href="#ref_6">6</a>].<p>While we have been actively promoting artifact sharing for the past 10 years
since the MILEPOST project&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_42">42</a>], it is still relatively rare in the community
systems community.
We have begun to understand possible reasons for that through our Artifact
Evaluation initiative&nbsp;[<a href="#ref_7">7</a>, <a href="#ref_8">8</a>] 
at PPoPP, CGO, PACT, SuperComputing and other leading ACM and IEEE conferences
which has attracted over a hundred of artifacts in the past few years.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_convert_to_ck"><b>Figure 2</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/4094a9fad1c48a5e-cropped.png" width="900"><br>
     <br><i>
       Reorganizing ad-hoc experimental packs into reusable, customizable and discoverable components
       with JSON API and meta information using the Collective Knowledge framework.
     </i><br><br>

     <!-- CK_LABEL={fig_convert_to_ck} -->


   </div></center>

   <p>Unfortunately, nearly all the artifacts have been shared simply as zip
archives, GitHub/GitLab/Bitbucket repositories, or VM/Docker images, with many
ad-hoc scripts to prepare, run and visualize experiments, as shown in
Figure&nbsp;<a href="#fig_convert_to_ck">2</a>a.
While a good step towards reproducibility, such ad-hoc artifacts are hard to
reuse and customize as they do not provide a common API and meta information.
<p>Some popular and useful services such as Zenodo&nbsp;[<a href="#ref_43">43</a>] and
FigShare&nbsp;[<a href="#ref_44">44</a>] allow researchers to upload individual artifacts to
specific websites while assigning DOI&nbsp;[<a href="#ref_45">45</a>] and providing some meta
information.
This helps the community to discover the artifacts, but does not necessarily
make them easy to reuse.
<p>After an ACM workshop on reproducible research methodologies (TRUST'14)&nbsp;[<a href="#ref_46">46</a>]
and a Dagstuhl Perspective workshop on Artifact Evaluation&nbsp;[<a href="#ref_8">8</a>],
we concluded that compute systems research lacked a common experimental
framework in contrast with other sciences&nbsp;[<a href="#ref_47">47</a>].
<p>Together with our fellow researchers, we also assembled the following wish-list
for such a framework:
<p>
<ul>

<p>
<li>
 it should be able to help researchers quickly organize their local code
and data into discoverable and reusable components with a unique ID, common API
and unified meta information, rather than being forced to upload them to the
web from the start;
<p>
<li>
 it should be open-source with a permissive license to simplify technology
transfer;
<p>
<li>
 it should be portable, simple to install and use from the command line;
<p>
<li>
 it should allow to assemble experimental workflows by simply plugging in
shared components;
<p>
<li>
 it should support native non-virtualized execution of such workflows,
i.e. not only via Virtual Machine&nbsp;[<a href="#ref_48">48</a>] and
Docker&nbsp;[<a href="#ref_49">49</a>], critical for empirical program optimization and hardware
co-design experiments;
<p>
<li>
 it should be able to adapt to continuously evolving software environments
and support different versions of tools such as rapidly evolving
compilers and libraries;
<p>
<li>
 it should include a local web server to simplify crowdsourcing of
experiments and visualization of results in workgroups.
<p></ul>

<p>Since there was no available open-source framework with all these features,
we decided to develop such a framework, Collective Knowledge (CK)&nbsp;[<a href="#ref_34">34</a>, <a href="#ref_35">35</a>], 
from scratch with initial support from the EU-funded TETRACOM project&nbsp;[<a href="#ref_50">50</a>].
CK is implemented as a small and portable Python module with a command line
front-end to assist users in converting their local objects (code and data)
into searchable, reusable and shareable directory entries with user-friendly
aliases and auto-generated Unique ID, JSON API and JSON meta
information&nbsp;[<a href="#ref_36">36</a>], as described in&nbsp;[<a href="#ref_35">35</a>, <a href="#ref_51">51</a>] and 
conceptually shown in Figure&nbsp;<a href="#fig_convert_to_ck">2</a>b.
<p>The user first creates a new local CK repository as follows:


<pre>$ ck add repo:new-ck-repo</pre>


<p>Initially, it is just an empty directory:


<pre>$ ck find repo:new-ck-repo</pre> 

<pre>$ ls `ck find repo:new-ck-repo`</pre>


<p>Now, the user starts adding research artifacts as CK components with extensible APIs.
For example, after noticing that we always perform 3 common actions on all our benchmarks
during our experiments, "compile", "run" and "autotune", we want to provide a common
API for these actions and benchmarks, rather than writing ad-hoc scripts.
The user can provide such an API with actions by adding a new CK module to a CK repository as follows:


<pre>$ ck add new-ck-repo:module:program</pre>


CK will then create two levels of directories <i>module</i> and <i>program</i> in the <i>new-ck-repo</i>
and will add a dummy <i>module.py</i> where common object actions can be implemented later.
CK will also create a sub-directory <i>.cm</i> (collective meta) 
with an automatically generated Unique ID of this module and various pre-defined 
descriptions in JSON format (date and time of module creation, author, license, etc)
to document provenance of the CK artifacts.
<p>Users can now create holders (directories) for such objects sharing common CK module and an API as follows:


<pre>$ ck add new-ck-repo:program:new-benchmark</pre>


CK will again create two levels of directories: 
the first one specifying used CK module (<i>program</i>) 
and the second one with alias <i>new-benchmark</i> 
to keep objects.
CK will also create three files in an internal <i>.cm</i> directory:
<p>
<ul>

<p>
<li>
 <b>meta.json</b> - an empty JSON file which can be gradually extended 
to describe a given object (such as added program in our example);
<p>
<li>
 <b>info.json</b> - a JSON file with the date and time of the last modification
as well as license, copyright and author information to keep attribution of all updates
for open research;
<p>
<li>
 <b>desc.json</b> - an empty JSON file to describe types of keys in <b>meta.json</b>
(useful for automatic type checking) and their value ranges (useful for autotuning
as we will show later in this report).
<p></ul>

<p>Users can then find a path to a newly created object holder (CK entry) using
the <i>ck find program:new-benchmark</i> command and then copy all files and
sub-directories related to the given object using standard OS shell commands.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_workflows"><b>Figure 3</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/bb46e3bcbfa20c5f-cropped.png" width="750"><br>
     <br><i>
        Converting ad-hoc scripts, tools and workflows to CK Python modules
        and standardized directories with actions, unified JSON API, and JSON meta information.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_workflows} -->


   </div></center>

   <p>This allows to get rid of ad-hoc scripts by implementing actions inside
reusable CK Python modules as shown in Figure&nbsp;<a href="#fig_ck_workflows">3</a>.
For example, the user can add an action to a given module such as <i>compile program</i>
as follows:


<pre>$ ck add_action module:program --func=compile</pre>


CK will create a dummy function body with an input dictionary <i>i</i>
inside <i>module.py</i> in the CK <i>module:program</i> entry.
Whenever this function is invoked via CK using the following format:


<pre>$ ck compile program:some_entry --param1=val1</pre>


the command line will be converted to <i>i</i> dictionary
and printed to the console to help novice users understand the CK API.
The user can now substitute this dummy function with a specific action on a specific entry
(some program in our example based on its meta information)
as conceptually shown in Figure&nbsp;<a href="#fig_ck_workflows">3</a>.
The above example shows how to call CK functions from Python modules rather than from the command line
using the <i>ck.access</i> function.
It also demonstrates how to find a path to a given <i>program</i> entry,
load its meta information and unique ID.
For the reader's convenience, Figure&nbsp;<a href="#fig_ck_commands">4</a> lists several important CK commands.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_commands"><b>Figure 4</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/ef7349a78679ca71-cropped.png" width="600"><br>
     <br><i>
        Main CK commands to create new or pull existing repositories, add modules, manage entries, perform actions, and use a local web server.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_commands} -->


   </div></center>

   <p>This functionality should be enough to start implementing unified compilation and execution
of shared programs.
For example, the <i>program</i> module can read instructions about how to compile and run
a given program from the JSON meta data of related entries, prepare and execute portable sub-scripts,
collect various statistics, and embed them to the output dictionary in a unified way.
This can be also gradually extended to include extra tools into compilation and execution workflow
such as code instrumentation and profiling.
<p>Here we immediately face another problem common for computer systems research:
how to support multiple versions of various and continuously evolving tools and libraries?
However, since we no longer hardwire calls to specific tools directly in scripts
but invoke them from higher-level CK modules, we can detect all required tools
and set up their environment before execution.
To support this concept even better, we have developed a cross-platform package manager
as a <a href="https://github.com/ctuning/ck-env">ck-env</a> repository&nbsp;[<a href="#ref_52">52</a>] with several CK modules
including <i>soft</i>, <i>env</i>, <i>package</i>, <i>os</i> and <i>platform</i>.
These modules allow the community to describe various operating systems (Linux, Windows, MacOS, Android);
detect platform features (<i>ck detect platform</i>); detect multiple-versions of
already installed software (<i>ck detect soft:compiler.gcc</i>);
prepare CK entries with their environments for a given OS and platform
using <i>env</i> module (<i>ck show env</i>) thus allowing easy co-existence of multiple versions of a given tool;
install missing software using <i>package</i> modules;
describe software dependencies using simple tags
in a program meta description (such as <i>compiler,gcc</i> or <i>lib,caffe</i>),
and ask the user to select an appropriate version during program compilation when multiple software
versions are registered in the CK as shown in Figure&nbsp;<a href="#fig_portable_package_manager">5</a>.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_portable_package_manager"><b>Figure 5</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/01714ac89d5bd629-cropped.png" width="800"><br>
     <br><i>
   CK modules implementing portable package manager with JSON API to enable cross-platform CK workflows.
   The community shares CK entries with Python scripts and JSON meta information via Git repositories
   to describe how to detect, build and install any software. This approach also simplify
   co-existence of multiple versions of the same tool.
     </i><br><br>

     <!-- CK_LABEL={fig_portable_package_manager} -->


   </div></center>

   <p>Such approach extends the concept of package managers including
Spack&nbsp;[<a href="#ref_53">53</a>] and EasyBuild&nbsp;[<a href="#ref_54">54</a>]
by integrating them directly with experimental CK workflows while using unified CK API,
supporting any OS and platform, and allowing the community to gradually extend existing
detection or installation procedures via CK Python scripts and CK meta data.
<p>Note that this CK approach encourages reuse of all such existing CK modules
from shared CK repositories rather then writing numerous ad-hoc scripts.
It should indeed be possible to substitute most of ad-hoc scripts
from public research projects (Figure&nbsp;<a href="#fig_convert_to_ck">2</a>)
with just a few above modules and entries (Figure&nbsp;<a href="#fig_ck_repo">6</a>),
and then collaboratively extend them, thus dramatically improving research productivity.
For this reason, we keep track of all publicly shared modules and their repositories
in this <a href="https://github.com/ctuning/ck/wiki/Shared-modules">wiki page</a>.
The user will just need to add/update a <i>.ckr.json</i> file
in the root directory of a given CK repository to describe a dependency
on other existing CK repositories with required modules or entries.
Since it is possible to uniquely reference any CK entry by two Unique IDs
(<i>module UID:object UID</i>), we also plan to develop a simple web service
to automatically index and discover all modules similar to DOI.
<p>
      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_repo"><b>Figure 6</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/57ab625c176c52ad-cropped.png" width="400"><br>
     <br><i>
        Typical experiment pack with reusable and discoverable components 
        shared in the CK format with two level directory structure (module and data).
     </i><br><br>

     <!-- CK_LABEL={fig_ck_repo} -->


   </div></center>

   <p>The open, file-based format of CK repositories allows researchers
to continue editing entries and their meta directly using their favourite editors.
It also simplifies exchange of these entries using Git repositories, zip archives, Docker images 
and any other popular tool.
At the same time, schema-free and human readable Python dictionaries
and JSON files helps users to collaboratively extend actions, API and meta information
while keeping backward compatibility.
Such approach should let the community to gradually and collaboratively convert and
cross-link all existing ad-hoc code and data into unified components
with extensible API and meta information.
This, in turn, allows users organize their own research while reusing existing artifacts,
building upon them, improving them and continuously contributing back to Collective Knowledge
similar to Wikipedia.
<p>We also noticed that CK can help students reduce preparation time 
for Artifact Evaluation&nbsp;[<a href="#ref_7">7</a>] at conferences while automating preparation 
and validation of experiments  since all artifacts, workflows and repositories are immediatelly ready
to be shared, ported and plugged in to research workflows.
<p>For example, the highest ranked artifact from
the CGO'17 article&nbsp;[<a href="#ref_55">55</a>]
was implemented and shared using the CK framework&nbsp;[<a href="#ref_56">56</a>].
That is why CK is now used and publicly extended by leading
companies&nbsp;[<a href="#ref_35">35</a>],
universities&nbsp;[<a href="#ref_55">55</a>]
and organizations&nbsp;[<a href="#ref_57">57</a>] to encourage, support and simplify
technology transfer between academia and industry.
<p>



<a name="sec_autotuning">

<!-- CK_CUR_SECTION={3} -->

<h2>3&nbsp;&nbsp;Assembling portable and customizable autotuning workflow</h2>

<!-- CK_LABEL={sec_autotuning} -->



Autotuning combined with various run-time adaptation,
genetic and machine learning techniques is a popular approach
in computer systems research to automatically explore multi-dimensional 
design and optimization spaces&nbsp;[<a href="#ref_10">10</a>, <a href="#ref_58">58</a>, <a href="#ref_59">59</a>, <a href="#ref_60">60</a>, <a href="#ref_11">11</a>, <a href="#ref_12">12</a>, <a href="#ref_13">13</a>, <a href="#ref_61">61</a>, <a href="#ref_14">14</a>, <a href="#ref_62">62</a>, <a href="#ref_15">15</a>, <a href="#ref_16">16</a>, <a href="#ref_17">17</a>, <a href="#ref_18">18</a>, <a href="#ref_19">19</a>, <a href="#ref_63">63</a>, <a href="#ref_20">20</a>, <a href="#ref_64">64</a>, <a href="#ref_65">65</a>, <a href="#ref_66">66</a>, <a href="#ref_67">67</a>, <a href="#ref_68">68</a>, <a href="#ref_69">69</a>, <a href="#ref_70">70</a>, <a href="#ref_29">29</a>, <a href="#ref_71">71</a>, <a href="#ref_72">72</a>, <a href="#ref_73">73</a>].
<p>CK allows to unify such techniques by developing
a common, universal, portable, customizable, multi-dimensional
and multi-objective autotuning workflow as a CK module
(<i>pipeline</i>
from the public
<a href="https://github.com/ctuning/ck-autotuning">ck-autotuning</a> repository
with the <i>autotune</i> function).
This allows us to abstract autotuning by decoupling it from the autotuned objects
such as <i>"program"</i>.
Users just need to provide a compatible function <i>"pipeline"</i> in a CK module 
which they want to be autotuned with a specific API including the following keys in both input and output:

<ul>

<p>
<li>
 <b>dependencies</b> to describe software dependencies via portable package manager from the CK;
<p>
<li>
 <b>choices</b> to expose various design and optimization knobs <b>c</b> such as algorithmic parameters, model topology, source-to-source transformations, compiler flags, hardware configurations, etc.;
<p>
<li>
 <b>characteristics</b> to monitor optimized behavior <b>b</b> such as execution time, code size, compilation time, energy, memory usage, accuracy, resiliency, costs, etc.;
<p>
<li>
 <b>features</b> to expose various object features <b>f</b> such as semantic program and data set features, hardware counters, platform properties, etc.;
<p>
<li>
 <b>state</b> to define run-time system state <b>s</b> such as hardware frequencies, network status, cache state, etc.
<p></ul>

<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_universal_autotuning_workflow"><b>Figure 7</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/9a4de89400e0ff21-cropped.png" width="800"><br>
     <br><i>
       Chaining together various CK modules with JSON API and JSON meta information
       to implement universal, portable, customizable, multi-dimensional
       and multi-objective autotuner gradually extended by the community.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_universal_autotuning_workflow} -->


   </div></center>

<p>Autotuning can now be implemented as a universal and extensible workflow
applied to any object with a matching JSON API by chaining together
related CK modules with various exploration strategies,
program transformation tools, compilers,
program compilation and execution pipeline, architecture simulators,
statistical analysis, Pareto frontier filter and other components,
as conceptually shown in Figure&nbsp;<a href="#fig_ck_universal_autotuning_workflow">7</a>.
Researchers can also use unified machine learning CK modules
(wrappers to R and scikit-learn&nbsp;[<a href="#ref_74">74</a>])
to model the relationship between <b>c</b>, <b>f</b>, <b>s</b>
and the observed behavior&nbsp;<b>b</b>, increase coverage, speed up (focus) exploration,
and predict efficient optimizations&nbsp;[<a href="#ref_4">4</a>, <a href="#ref_6">6</a>].
They can also take advantage of a universal complexity reduction module
which can automatically simplify found solutions without changing their behavior,
reduce models and features without sacrificing accuracy,
localize performance issues via differential analysis&nbsp;[<a href="#ref_75">75</a>],
reduce programs to localize bugs, and so on.
<p>Even more importantly, our concept of a universal autotuning workflow,
knowledge sharing and artifact reuse can help teach students
how to apply a well-established holistic and top-down
experimental methodology from natural sciences to continuously 
learn and improve the behavior of complex computer systems&nbsp;[<a href="#ref_35">35</a>, <a href="#ref_4">4</a>].
Researchers can continue exposing more design and optimization knobs&nbsp;<b>c</b>,
behavioral characteristics&nbsp;<b>b</b>, static and dynamic features&nbsp;<b>f</b>,
and run-time state <i>state</i> to optimize and model behavior
of various interconnected objects from the workflow depending on their
research interests and autotuning scenarios.
<p>Such scenarios are also implemented as CK modules
and describe which sets of choices to select, 
how to autotune them and which multiple characteristics to trade off.
For example, existing scenarios include
"autotuning OpenCL parameters to improve execution time",
"autotuning GCC flags to balance execution time and code size",
"autotune LLVM flags to reduce execution time",
"automatically fuzzing compilers to detect bugs",
"exploring CPU and GPU frequency in terms of execution time and power consumption",
"autotuning deep learning algorithms in terms of speed, accuracy, energy, memory usage and costs",
and so on.
<p>You can see some of the autotuning scenarios using the following commands:



<pre>$ ck pull repo:ck-crowdtuning</pre>

<pre>$ ck search module --tags="program optimization"</pre>

<pre>$ ck list program</pre>



They can then be invoked from the command line as follows:



<pre>$ ck autotune program:[CK program alias] --scenario=[above CK scenario alias]</pre>


<p>



<a name="sec_flag_autotuning">

<!-- CK_CUR_SECTION={4} -->

<h2>4&nbsp;&nbsp;Implementing universal compiler flag autotuning</h2>

<!-- CK_LABEL={sec_flag_autotuning} -->



In this section we would like to show how to 
customize our universal autotuning workflow
to tackle an old but yet unsolved problem 
of finding the most efficient 
selection of compiler flag which minimizes 
program size and execution time.
<p>Indeed, the raising complexity of ever changing hardware 
made development of compilers very challenging.
Popular GCC and LLVM compilers nowadays include hundreds 
of optimizations (Figure&nbsp;<a href="#fig_rising_compiler_flags">8</a>) 
and often fail to produce efficient code (execution time and code size)
on realistic workloads within a reasonable compilation time&nbsp;[<a href="#ref_10">10</a>, <a href="#ref_58">58</a>, <a href="#ref_76">76</a>, <a href="#ref_77">77</a>, <a href="#ref_4">4</a>].
Such large design and optimization spaces mean
that hardware and compiler designers can afford to explore
only a tiny fraction of the whole optimization space 
using just few ad-hoc benchmarks and data sets on a few architectures
in a tough mission to assemble <i>-O3</i>, <i>-Os</i> and other
optimization levels across all supported architectures and workloads.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_rising_compiler_flags"><b>Figure 8</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/aaf72841d69bde12-cropped.png" width="500"><br>
     <br><i>
       Continuously rising number of boolean and parametric optimization flags 
       in GCC over years (obtained by automatically parsing GCC source code 
       and manual pages, therefore small variation is possible).
     </i><br><br>

     <!-- CK_LABEL={fig_rising_compiler_flags} -->


   </div></center>

<p>Our idea is to keep compiler as a simple collection of code analysis and transformation routines 
and separate it from optimization heuristics.
In such case we can use CK autotuning workflow to collaboratively optimize multiple 
shared benchmarks and realistic workloads across diverse hardware, exchange optimization results,
and continuously learn and update compiler optimization heuristics for a given hardware 
as a compiler plugin.
We will demonstrate this approach by randomly optimizing compiler flags 
for <i>susan corners</i> program with aging <i>GCC 4.9.2</i>, the latest <i>GCC 7.1.0</i>
and compare them with&nbsp;<i>Clang 3.8.1</i>.
We already monitor and optimize execution time and code size of this popular image processing 
application across different compilers and platforms for many years&nbsp;[<a href="#ref_24">24</a>].
That is why we are interested to see if we can still improve it with the CK autotuner 
on the latest <i>Raspberry Pi 3 (Model B)</i> devices (RPi3) extensively used for educational purposes.
<p>First of all, we added <i>susan</i> program with <i>corners</i> algorithm 
to the <i>ctuning-programs</i> repository with the JSON meta information 
describing compilation and execution 
as shown in Figure&nbsp;<a href="#fig_susan_corners_ck_json_meta">9</a>.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_susan_corners_ck_json_meta"><b>Figure 9</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/354165ad6dde5667-cropped.png" width="400"><br>
     <br><i>
       CK JSON meta information for susan corners (image processing program) to describe software dependencies as well as how to compile and run it.
     </i><br><br>

     <!-- CK_LABEL={fig_susan_corners_ck_json_meta} -->


   </div></center>

<p>We can then test its compilation and execution by invoking the program pipeline as following:


<pre>$ ck pipeline program:cbench-automotive-susan</pre>



<p>CK program pipeline will first attempt to detect platform features
(OS, CPU, GPU) and embed them to the input dictionary using key <i>features</i>.
Note that in case of cross-compilation for a target platform different from the host one
(Android, remote platform via SSH, etc), 
it is possible to specify such platform using CK <i>os</i> entries and <i>--target_os=</i> flag.
<p>For example, it is possible to compile and run a given CK program for Android via adb as following:


<pre>$ ck ls os</pre>

<pre>$ ck pipeline program:cbench-automotive-susan --target_os=android21-arm64</pre>



<p>Next, CK will try to resolve software dependencies and prepare environment for compilation
by detecting already installed compilers using CK <i>soft:compiler.*</i> entries 
or installing new ones if none was found using CK <i>package:compiler.*</i>.
Each installed compiler for each target 
will have an associated CK entry with prepared environment 
to let computer systems researchers work with different 
versions of different tools:


<pre>$ ck show env</pre>

<pre>$ ck show env --target_os=android21-arm64</pre>

<pre>$ ck show env --tags=compiler</pre>



<p>Automatically detected version of a selected compiler is used by CK
to find and preload all available optimization flags 
from related <i>compiler:*</i> entries to the <i>choices</i> key
of a pipeline input.
An example of such flags and tags in the CK JSON format 
for GCC 4.9 is shown in Figure&nbsp;<a href="#fig_ck_gcc_meta">10</a>.
The community can continue extending such descriptions for different compilers
including <i>GCC, LLVM, Julia, Open64, PathScale, Java, MVCC, ICC and PGI</i>
using either public <a href="https://github.com/ctuning/ck-autotuning">ck-autotuning</a> repository
or their own ones.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_gcc_meta"><b>Figure 10</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/84964b0640173783-cropped.png" width="400"><br>
     <br><i>
       CK JSON description of compiler flags for GCC 4.9 to enable universal autotuning.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_gcc_meta} -->


   </div></center>

<p>Finally, CK program pipeline compiles a given program, runs it on a target platform
and fills in sub-dictionary <i>characteristics</i> 
with compilation time, object and binary sizes, MD5 sum of the binary, execution time,
used energy (if supported by a used platform), and all other obtained measurements
in the common pipeline dictionary.
<p>We are now ready to implement universal compiler flag autotuning coupled with
this program pipeline.
For a proof-of-concept, we implemented GCC compiler flags exploration strategy
which automatically generate N random combinations of compiler flags, 
compile a given program with each combination, runs it and record all results 
(inputs and outputs of a pipeline) in a reproducible form in a <i>local</i> 
CK repository using <i>experiment</i> module from 
the <a href="https://github.com/ctuning/ck-analytics">ck-analytics</a> 
repository:
<p>

<pre>$ ck pull repo:ck-crowdtuning</pre>

<pre>$ ck info module:experiment.tune.compiler.flags.gcc</pre>



<p>The JSON meta information of this module describes which keys to select
in the program pipeline, how to tune them, and which characteristics to monitor
and record as shown in Figure&nbsp;<a href="#fig_ck_gcc_tuning_meta">11</a>.
Note that a string starting with <i>\#\#</i> is used to reference any key
in a complex, nested JSON or Python dictionary (<i>CK flat key</i> [<a href="#ref_4">4</a>]).
Such <i>flat key</i> always starts with <i>\#</i> 
followed by <i>\#key</i> if it is a dictionary key or
<i>@position_in_a_list</i> if it is a value in a list. 
CK also supports wild cards in such flat keys 
such as <i>"\#\#compiler_flags\#\*"</i> and <i>"\#\#characteristics\#\*</i>
to be able to select multiple sub-keys, dictionaries 
and lists in a given dictionary.
<p>
      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_gcc_tuning_meta"><b>Figure 11</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/4c2fb19c5b0fd89a-cropped.png" width="400"><br>
     <br><i>
       CK JSON description of random autotuning of compiler flags applied to program pipeline.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_gcc_tuning_meta} -->


   </div></center>

<p>We can now invoke this CK experimental scenario from the command line as following:
<p>

<pre>$ ck autotune program:cbench-automotive-susan --iterations=300 --repetitions=3 
  --scenario=experiment.tune.compiler.flags.gcc
  --cmd_key=corners --record_uoa=tmp-susan-corners-gcc4-300-rnd</pre>


<p>CK will generate 300 random combinations of compiler flags, compile <i>susan corners</i> program 
with each combination, run each produced code 3 times to check variation, and record
results in the <i>experiment:tmp-susan-corners-gcc4-300-rnd</i>.
We can now visualize these autotuning results using the following command line:


<pre>$ ck plot graph:tmp-susan-corners-gcc4-300-rnd</pre>


<p>Figure&nbsp;<a href="#fig_autotuning_susan_gcc4">12</a> shows a manually annotated graph 
with the outcome of GCC 4.9.2 random compiler flags autotuning 
applied to susan corners on an RPi3 device in terms of execution 
time with variation and code size.
Each blue point on this graph is related to one combination of random compiler flags.
The red line highlights the frontier of all autotuning results (not necessarily Pareto optimal) 
which trade off execution time and code size during multi-objective optimization.
We also plotted points when default GCC compilation is used without any flags 
or with <i>-O3</i> and <i>-Os</i> optimization levels.
Finally, we decided to compare optimization results with <i>Clang 3.8.1</i> also available on RPi3.
<p>    
  
  
  
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_susan_gcc4"><b>Figure 12</b>

    
      <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/4373f49dea7663db-cropped.png" width="900"><br>
     <br>
      <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f9e6ec8d198c36c3&subpoint=7cf654adb86fb606">A1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   11.7&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   60560
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:0b867dd820354a8b&subpoint=5734f47e4214a783">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   4.3&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   36360
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:b2b26ab783304fc4&subpoint=7d87c22a2425da10">A3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   6.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   32184
  </td>
  <td valign="top">
   <small>   -Os
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:98688a71f99ac30b&subpoint=4bcd9dad6b249a79">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   4.2&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   32448
  </td>
  <td valign="top">
   <small>   -O3 -fno-guess-branch-probability -fno-if-conversion -fno-ivopts -fno-schedule-insns -fsingle-precision-constant --param max-unswitch-insns=5
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:984b2d8abc3c4415&subpoint=78c281b4cab897a6">A5R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   3.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   33376
  </td>
  <td valign="top">
   <small>   -O3 -fbranch-probabilities -fno-ivopts -fno-sched-dep-count-heuristic
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:7af17ca204080b57&subpoint=5a464ecf81b60098">A6R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   3.4&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   33804
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline-small-functions -fno-ivopts -fno-tree-partial-pre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:a32e34c31b900930&subpoint=64ec888d2e6a0669">A7</a>
   </b>  </td>
  <td valign="top">
   CLANG&nbsp;3.8.1
  </td>
  <td valign="top">
   11.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   58368
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:99141b3313132494&subpoint=a63f42ac837e38d0">A8</a>
   </b>  </td>
  <td valign="top">
   CLANG&nbsp;3.8.1
  </td>
  <td valign="top">
   4.5&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   35552
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
</table>
<p>     <br>
    
$#ck_include_start#$
{"cid":"2d41f89bcf32d4d4:6b6d77a51c74ec1a", "where":"div#ck_interactive_98fd5cefd4ef2b27", "html":"rpi3-autotuning-susan-gcc4-interactive.html", "style":"rpi3-autotuning-susan-gcc4-interactive.style", "add_div":"yes", "add_box":"yes", "box_width":700, "remove_script_src":"no"}
$#ck_include_stop#$
<p>    <br><i>
      Results of GCC 4.9.2 random compiler flag autotuning of susan corners program on Raspberry&nbsp;Pi&nbsp;3 (Model&nbsp;B) 
      device using CK with a highlighted frontier (trading-off execution time and code size) 
      and best found combinations of flags on this frontier.
    </i><br><br>

    <!-- CK_LABEL={fig_autotuning_susan_gcc4} -->


  </div></center>

<p>Besides showing that <i>GCC -O3</i> (optimization choice <b>A2</b>)
and <i>Clang -O3</i> (optimization choice <b>A8</b>) can produce a very similar code, 
these results confirm well that it is indeed possible to automatically obtain execution time 
and binary size of <i>-O3</i> and <i>-Os</i> levels in comparison with non-optimized code 
within tens to hundreds autotuning iterations (green improvement vectors with &nbsp;3.6x execution time speedup 
and &nbsp;1.6x binary size improvement).
The graph also shows that it is possible to improve best optimization level <i>-O3</i> 
much further and obtain &nbsp;1.3x execution time speedup (optimization solution <b>A6R</b>
or obtain 11&#37; binary size improvement without sacrifying original execution time
(optimization solution <b>A4R</b>).
Such automatic squeezing of a binary size without sacrificing performance 
can be very useful for the future IoT devices.
<p>Note that it is possible to browse all results in a user-friendly way 
via web browser using the following command:
<p>

<pre>$ ck browse experiment:tmp-susan-corners-gcc4-300-rnd</pre>


<p>CK will then start internal CK web server 
available in the <a href="https://github.com/ctuning/ck-web">ck-web</a>
repository, will run a default web browser, and will 
open a web page with all given experimental results.
Each experiment on this page has an associated button 
with a command line to replay it via CK such as:                          
<p>

<pre>$ ck replay experiment:7b41a4ac1b3b4f2b --point=00e81f4e4abb371d</pre>


<p>CK will then attempt to reproduce this experiment using the same input
and then report any differences in the output.
This simplifies validation of shared experimental results 
(optimizations, models, bugs) by the community
and possibly with a different software and hardware setup
(CK will automatically adapt the workflow to a user platform).
<p>We also provided support to help researchers 
visualize their results as interactive graphs 
using popular D3.js library as demonstrated in this 
<a href="http://cknowledge.org/repo/web.php?wcid=graph:6b6d77a51c74ec1a&subgraph=rpi3-autotuning-susan-gcc4-interactive">link</a>.
<p>Looking at above optimization results one may notice 
that one of the original optimization solutions on a frontier <b>A4</b> 
has &nbsp;40 optimization flags, while <b>A4R</b> only 7 as shown in Table&nbsp;<a href="#fig_autotuning_susan_gcc4_reduce">1</a>.
The natural reason is that not all randomly selected flags contribute to improvements.
That is why we developed a simple and universal complexity reduction algorithm.
It iteratively and randomly removes choices from a found solution one by one
if they do not influence monitored characteristics such as execution time and code size
in our example.
<p>    
  
  <center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_susan_gcc4_reduce"><b>Table 1</b><br><br>

    
      <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:8556dbf4e51a825d&subpoint=86ca1630895041c1">A4</a>
   </b>  </td>
  <td valign="top">
   <small>   -O3 -fira-algorithm=priority -fcaller-saves -fno-devirtualize-speculatively -fno-function-cse -fgcse-sm -fno-guess-branch-probability -fno-if-conversion -fno-inline-functions-called-once -fipa-reference -fno-ira-loop-pressure -fira-share-save-slots -fno-isolate-erroneous-paths-dereference -fno-ivopts -floop-nest-optimize -fmath-errno -fmove-loop-invariants -fsched-last-insn-heuristic -fsched2-use-superblocks -fno-schedule-insns -fno-signed-zeros -fsingle-precision-constant -fno-tree-sink -fno-unsafe-loop-optimizations --param asan-instrument-reads=1 --param gcse-unrestricted-cost=5 --param l1-cache-size=11 --param large-function-growth=33 --param loop-invariant-max-bbs-in-loop=636 --param max-completely-peel-loop-nest-depth=7 --param max-delay-slot-live-search=163 --param max-gcse-insertion-ratio=28 --param max-inline-insns-single=282 --param max-inline-recursive-depth-auto=0 --param max-jump-thread-duplication-stmts=6 --param max-last-value-rtl=4062 --param max-pipeline-region-insns=326 --param max-sched-region-blocks=17 --param max-tail-merge-iterations=2 --param max-unswitch-insns=5 --param max-vartrack-expr-depth=6 --param min-spec-prob=1 --param omega-eliminate-redundant-constraints=1 --param omega-max-keys=366 --param omega-max-wild-cards=36 --param sms-dfa-history=0
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:98688a71f99ac30b&subpoint=4bcd9dad6b249a79">A4R</a>
   </b>  </td>
  <td valign="top">
   <small>   -O3 -fno-guess-branch-probability -fno-if-conversion -fno-ivopts -fno-schedule-insns -fsingle-precision-constant --param max-unswitch-insns=5
   </small>  </td>
 </tr>
</table>
<p>    <br><i>
      One of original optimization solutions found after autotuning with random selection of compiler flags (A4) 
      and reduced optimization solution (A4R) which results in the same or better execution time and code size.
    </i><br><br>

    <!-- CK_LABEL={fig_autotuning_susan_gcc4_reduce} -->


  </div></center>

<p>Such complexity reduction (pruning) of an existing solution can be invoked as following
(flag <i>--prune_md5</i> tells CK to exclude a given choice without running code
if MD5 of a produced binary didn't change, thus considerably speeding up flag pruning):
<p>

<pre>$ck replay experiment:93974bf451f957eb --point=74e9c9f14b424ba7 --prune --prune_md5 @prune.json</pre>


<p>The <i>'prune.json'</i> file describes conditions on program pipeline keys 
when a given choice should be removed as shown in Figure&nbsp;<a href="#fig_ck_pruning_meta">13</a>.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_pruning_meta"><b>Figure 13</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/e2a106816a4c5093-cropped.png" width="400"><br>
     <br><i>
       CK JSON description of conditions on choices in a pipeline input to reduce choices from a found optimization solution.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_pruning_meta} -->


   </div></center>

<p>Such universal complexity reduction approach helps software engineers better understand
individual contribution of each flag to improvements or degradations of all monitored
characteristics such as execution time and code size as shown in Figure&nbsp;<a href="#fig_ck_pruning_contribution">14</a>.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_pruning_contribution"><b>Figure 14</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/a472d964631c80a2-cropped.png" width="400"><br>
     <br><i>
       Contribution of individual compiler flags to improvements or degradations of monitored characteristics during universal complexity reduction.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_pruning_contribution} -->


   </div></center>

<p>Asked by compiler developers, we also provided an extension to our complexity reduction module 
to turn off explicitly all available optimization choices one by one
if they do not influence found optimization result.
Table&nbsp;<a href="#fig_autotuning_susan_gcc4_invert">2</a> demonstrates this approach and shows all compiler optimizations contributing to the found optimization solution.
It can help improve internal optimization heuristics, global optimization levels such as <i>-O3</i>,
and improve machine learning based optimization predictions.
This extension can be invoked by adding flags <i>--prune_invert --prune_invert_do_not_remove_key</i>
when reducing complexity of a given solution such as:


<pre>$ ck replay experiment:93974bf451f957eb --point=74e9c9f14b424ba7 --prune --prune_md5 --prune_invert --prune_invert_do_not_remove_key @prune.json</pre>


<p>    
  
  <center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_susan_gcc4_invert"><b>Table 2</b><br><br>

   
     <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:7af17ca204080b57&subpoint=5a464ecf81b60098">A6R</a>
   </b>  </td>
  <td valign="top">
   <small>   -O3 -fno-inline-small-functions -fno-ivopts -fno-tree-partial-pre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f594f90a1545babf&subpoint=de11e85953947388">A6RI</a>
   </b>  </td>
  <td valign="top">
   <small>   <b>-O3</b> -fno-inline-small-functions -fno-ivopts -fno-tree-bit-ccp -fno-tree-partial-pre -fno-tree-pta -fno-associative-math -fno-auto-inc-dec -fno-branch-probabilities -fno-branch-target-load-optimize -fno-branch-target-load-optimize2 -fno-caller-saves -fno-check-data-deps -fno-combine-stack-adjustments -fno-conserve-stack -fno-compare-elim <b>-fcprop-registers</b> <b>-fcrossjumping</b> <b>-fcse-follow-jumps</b> -fno-cse-skip-blocks -fno-cx-limited-range -fno-data-sections <b>-fdce</b> -fno-delayed-branch -fno-devirtualize -fno-devirtualize-speculatively -fno-early-inlining -fno-ipa-sra -fno-expensive-optimizations -fno-fat-lto-objects -fno-fast-math -fno-finite-math-only -fno-float-store <b>-fforward-propagate</b> -fno-function-sections -fno-gcse-after-reload -fno-gcse-las -fno-gcse-lm -fno-graphite-identity -fno-gcse-sm -fno-hoist-adjacent-loads -fno-if-conversion <b>-fif-conversion2</b> -fno-indirect-inlining -fno-inline-functions -fno-inline-functions-called-once -fno-ipa-cp -fno-ipa-cp-clone -fno-ipa-pta <b>-fipa-pure-const</b> -fno-ipa-reference -fno-ira-hoist-pressure -fno-ira-loop-pressure -fno-ira-share-save-slots <b>-fira-share-spill-slots</b> <b>-fisolate-erroneous-paths-dereference</b> -fno-isolate-erroneous-paths-attribute -fno-keep-inline-functions -fno-keep-static-consts -fno-live-range-shrinkage -fno-loop-block -fno-loop-interchange -fno-loop-strip-mine -fno-loop-nest-optimize -fno-loop-parallelize-all -fno-lto -fno-merge-all-constants -fno-merge-constants -fno-modulo-sched -fno-modulo-sched-allow-regmoves <b>-fmove-loop-invariants</b> -fno-branch-count-reg -fno-defer-pop -fno-function-cse <b>-fguess-branch-probability</b> <b>-finline</b> <b>-fmath-errno</b> -fno-peephole <b>-fpeephole2</b> -fno-sched-interblock -fno-sched-spec -fno-signed-zeros -fno-toplevel-reorder -fno-trapping-math -fno-zero-initialized-in-bss <b>-fomit-frame-pointer</b> -fno-optimize-sibling-calls -fno-partial-inlining -fno-peel-loops -fno-predictive-commoning -fno-prefetch-loop-arrays -fno-ree -fno-rename-registers <b>-freorder-blocks</b> -fno-reorder-blocks-and-partition -fno-rerun-cse-after-loop -fno-reschedule-modulo-scheduled-loops -fno-rounding-math -fno-sched2-use-superblocks <b>-fsched-pressure</b> -fno-sched-spec-load -fno-sched-spec-load-dangerous -fno-sched-group-heuristic <b>-fsched-critical-path-heuristic</b> -fno-sched-spec-insn-heuristic -fno-sched-rank-heuristic -fno-sched-dep-count-heuristic <b>-fschedule-insns</b> <b>-fschedule-insns2</b> -fno-section-anchors -fno-selective-scheduling -fno-selective-scheduling2 -fno-sel-sched-pipelining -fno-sel-sched-pipelining-outer-loops -fno-shrink-wrap -fno-signaling-nans -fno-single-precision-constant -fno-split-ivs-in-unroller -fno-split-wide-types -fno-strict-aliasing <b>-fstrict-overflow</b> -fno-tracer -fno-tree-builtin-call-dce -fno-tree-ccp <b>-ftree-ch</b> -fno-tree-coalesce-vars -fno-tree-copy-prop <b>-ftree-copyrename</b> <b>-ftree-dce</b> <b>-ftree-dominator-opts</b> -fno-tree-dse <b>-ftree-forwprop</b> -fno-tree-fre -fno-tree-loop-if-convert -fno-tree-loop-if-convert-stores <b>-ftree-loop-im</b> -fno-tree-phiprop -fno-tree-loop-distribution -fno-tree-loop-distribute-patterns -fno-tree-loop-linear <b>-ftree-loop-optimize</b> -fno-tree-loop-vectorize -fno-tree-pre <b>-ftree-reassoc</b> -fno-tree-sink <b>-ftree-slsr</b> <b>-ftree-sra</b> -fno-tree-switch-conversion -fno-tree-tail-merge <b>-ftree-ter</b> -fno-tree-vectorize <b>-ftree-vrp</b> -fno-unit-at-a-time -fno-unroll-all-loops -fno-unroll-loops -fno-unsafe-loop-optimizations -fno-unsafe-math-optimizations -fno-unswitch-loops -fno-variable-expansion-in-unroller -fno-vect-cost-model -fno-vpt -fno-web -fno-whole-program -fno-wpa <b>-fexcess-precision=standard</b> <b>-ffp-contract=off</b> <b>-fira-algorithm=CB</b> <b>-fira-region=all</b>
   </small>  </td>
 </tr>
</table>
<p>   <br><i>
     Explicitly switching off all compiler flags one by one if they do not influence the optimization result - 
     useful to understand all compiler optimizations which contributed to the found solution. 
   </i><br><br>

   <!-- CK_LABEL={fig_autotuning_susan_gcc4_invert} -->


  </div></center>

<p>We have been analyzing already aging <i>GCC 4.9.2</i> because 
it is still the default compiler for Jessy Debian distribution on RPi3.
However, we would also like to check how our universal autotuner
works with the latest <i>GCC 7.1.0</i>.
<p>Since there is no yet a standard Debian GCC 7.1.0 package available for RPi3,
we need to build it from scratch.
This is not a straightforward task since we have to pick up correct 
configuration flags which will adapt GCC build to quite outdated RPi3 libraries.
However, once we manage to do it, we can automate this process
using CK <i>package</i> module. 
<p>We created a public <a href="https://github.com/ctuning/ck-dev-compilers">ck-dev-compilers</a> repository
to automate building and installation of various compilers including GCC and LLVM via CK.
It is therefore possible to install GCC 7.1.0 on RPi3 as following 
(see Appendix or GitHub repository ReadMe file for more details):
<p>

<pre>$ ck pull repo:ck-dev-compilers 

$ ck install package:compiler-gcc-any-src-linux-no-deps --env.PARALLEL_BUILDS=1 --env.GCC_COMPILE_CFLAGS=-O0 --env.GCC_COMPILE_CXXFLAGS=-O0 --env.EXTRA_CFG_GCC=--disable-bootstrap --env.RPI3=YES --force_version=7.1.0</pre>


<p>This CK package has an <i>install.sh</i> script which is customized 
using environment variables or <i>--env</i> flags to build GCC for a target platform.
The JSON meta data of this CK package provides optional software dependencies 
which CK has to resolve before installation (similar to CK compilation).
If installation succeeded, you should be able to see two prepared environments
for GCC 4.9.2 and GCC 7.1.0 which now co-exist in the system.
<p>

<pre>$ ck show env --tags=gcc</pre>


<p>Whenever we now invoke CK autotuning, CK software and package manager 
will detect multiple available versions of a required software dependency
and will let you choose which compiler version to use.
<p>Let us now autotune the same <i>susan corners</i> program 
by generating 300 random combinations of <i>GCC 7.1.0</i> compiler flags
and record results in the <i>experiment:tmp-susan-corners-gcc7-300-rnd</i>:
<p>

<pre>$ ck autotune program:cbench-automotive-susan --iterations=300 --repetitions=3 
  --scenario=experiment.tune.compiler.flags.gcc
  --cmd_key=corners --record_uoa=tmp-susan-corners-gcc7-300-rnd</pre>


<p>Figure&nbsp;<a href="#fig_autotuning_susan_gcc7">15</a> shows the results of such <i>GCC 7.1.0</i>
compiler flag autotuning (<b>B</b> points) and compares them 
against <i>GCC 4.9.2</i> (<b>A</b> points).
Note that this graph is also available in interactive form&nbsp;<a href="http://cknowledge.org/repo/web.php?wcid=graph:96fd8e4c8394b1bc&subgraph=rpi3-autotuning-susan-gcc7-interactive">online</a>.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_susan_gcc7"><b>Figure 15</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/9e4b1594d3b99443-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:0b867dd820354a8b&subpoint=5734f47e4214a783">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   4.3&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   36360
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:984b2d8abc3c4415&subpoint=78c281b4cab897a6">A5R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   3.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   33376
  </td>
  <td valign="top">
   <small>   -O3 -fbranch-probabilities -fno-ivopts -fno-sched-dep-count-heuristic
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:7af17ca204080b57&subpoint=5a464ecf81b60098">A6R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   3.4&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   33804
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline-small-functions -fno-ivopts -fno-tree-partial-pre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:ea8ded6ddee6093a&subpoint=6006d3eadf403088">B1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   11.5&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   58008
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:00fa4e108053ac7b&subpoint=de68bdc517447085">B2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   3.2&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   34432
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:eeba1f30493b8bd4&subpoint=5d64cf89c67817be">B3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   4.4&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   29980
  </td>
  <td valign="top">
   <small>   -Os
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f4f13d284194463a&subpoint=5421ba4fdca2c4b6">B4</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   3.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   31460
  </td>
  <td valign="top">
   <small>   -O3 -fno-cx-fortran-rules -fno-devirtualize -fno-expensive-optimizations -fno-if-conversion -fira-share-save-slots -fno-ira-share-spill-slots -fno-ivopts -fno-loop-strip-mine -finline -fno-math-errno -frounding-math -fno-sched-rank-heuristic -fno-sel-sched-pipelining-outer-loops -fno-semantic-interposition -fsplit-wide-types -fno-tree-ccp -ftree-dse
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:2b6646924c79bf81&subpoint=e8a36aa042438771">B4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   3.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   31420
  </td>
  <td valign="top">
   <small>   -O3 -fno-expensive-optimizations -fno-ivopts -fno-math-errno
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Results of GCC 7.1.0 random compiler flag autotuning of susan corners program on Raspberry&nbsp;Pi&nbsp;3 (Model&nbsp;B) 
      device using CK with a highlighted frontier (trading-off execution time and code size), 
      best combinations of flags on this frontier, and comparison with the results from GCC 4.9.2.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_susan_gcc7} -->


   </div></center>

<p>It is interesting to see considerable improvement in execution time of susan corners 
when moving from GCC 4.9 to GCC 7.1 with the best optimization level <i>-O3</i>.
This graph also shows that new optimization added during the past 3 years opened up
many new opportunities thus considerably expanding autotuning frontier (light red
dashed line versus dark red dashed line).
Autotuning only managed to achieve a modest improvement of a few percent over <i>-O3</i>.
<p>On the other hand, GCC <i>-O3</i> and <i>-Os</i> are still far from achieving
best trade-offs for execution time and code size.
For example, it is still possible to improve a program binary size 
by &nbsp;10&#37; (reduced solution <b>B4R</b>) without degrading best achieved 
execution time with the <i>-O3</i> level (<b>-O3</b>), or improve 
execution time of <b>-Os</b> level by &nbsp;28&#37; while slightly degrading code size by &nbsp;5&#37;.
<p>Note that for readers' convenience we added scripts to reproduce and validate 
all results from this section to the following CK entries:
<p>

<pre>$ ck pull repo:ck-rpi-optimization-results 

$ ck find script:rpi3-susan*</pre>


<p>These results confirm that it is difficult to manually prepare compiler optimization
heuristic which can deliver good trade offs between execution time and code size 
in such a large design and optimization spaces.
They also suggest that either susan corners or similar code 
was eventually added to the compiler regression testing suite,
or some engineer check it manually and fixed compiler heuristic.
However, there is also no guarantee that future GCC versions will still
perform well on the susan corners program.
Neither these results guarantee that GCC 7.1.0
will perform well on other realistic workloads or devices.
<p>



<a name="sec_crowdtuning">

<!-- CK_CUR_SECTION={5} -->

<h2>5&nbsp;&nbsp;Crowdsourcing autotuning</h2>

<!-- CK_LABEL={sec_crowdtuning} -->



We use our universal CK autotuning workflow to teach students and end-users 
how to automatically find good trade offs between multiple characteristics 
for any individual program, data set, compiler, environment and hardware.
At the same time, automatically tuning many realistic workloads
is very costly and can easily take from days to weeks and months&nbsp;[<a href="#ref_24">24</a>].
<p>Common experimental frameworks can help tackle this problem too by 
crowdsourcing autotuning across diverse hardware provided by volunteers and combining it with online
classification, machine learning and run-time adaptation&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_66">66</a>, <a href="#ref_6">6</a>].
However, our previous frameworks did not cope well with "big data" problem
(cTuning framework&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_9">9</a>] based on MySQL database) 
or were too "heavy" (Collective Mind aka cTuning 3 framework&nbsp;[<a href="#ref_4">4</a>]).
<p>Extensible CK workflow framework combined with our cross-platform package manager, 
internal web server and machine learning, helped solve most of the above issues.
For example, we introduced a notion of a remote repository in the CK - 
whenever such repository is accessed CK simply forward all JSON requests 
to an appropriate web server.
<p>CK always has a default remote repository <i>remote-ck</i> connected
with a public optimization repository running CK web serve 
at&nbsp;<a href="cKnowledge.org/repo">cKnowledge.org/repo</a>: 
<p>

<pre>$ ck load repo:remote-ck --min</pre>


<p>For example, one can see publicly available experiments from command line as following:


<pre>$ ck list remote-ck:experiment:* | sort</pre>


<p>Such organization allows one to crowdsource autotuning, i.e. distributing autotuning 
of given shared workloads in a cloud or across diverse platforms simply by using remote 
repositories instead of local ones.
On the other hand, it does not address the problem of optimizing larger applications
with multiple hot spots.
It also does not solve the "big data" problem 
when a large amount of data from multiple participants
needed for reproducibility will be continuously aggregated in a CK server.
<p>However, we have been already addressing the first problem by either 
instrumenting, monitoring and optimizing hot code regions in large applications 
using our small "XOpenME" library, or even extracting such code regions 
from a large application with a run-time data set and registering them 
in the CK as standalone programs (codelets or computational species) 
as shown in Figure&nbsp;<a href="#fig_ck_codelets">16</a>
(&nbsp;[<a href="#ref_4">4</a>]).
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_codelets"><b>Figure 16</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/1b9b091003515e71-cropped.png" width="300"><br>
     <br><i>
       Preparing larger applications such as Firefox and Chrome for CK-based autotuning: 
       a) instrumenting, monitoring and optimizing hot code regions using "XOpenME" library 
       b) extracting code regions from a large application with a run-time data set and register them in the CK as standalone programs (codelets)
     </i><br><br>

     <!-- CK_LABEL={fig_ck_codelets} -->


   </div></center>

<p>In the MILEPOST project&nbsp;[<a href="#ref_24">24</a>] 
we used a proprietary "codelet extractor" tool from CAPS Entreprise 
(now dissolved) to automatically extract such hot spots with their data sets 
from several real software projects and 8 popular benchmark suits 
including NAS, MiBench, SPEC2000, SPEC2006, Powerstone, UTDSP and SNU-RT.
We shared those of them with a permissive license as CK programs 
in the <a href="https://github.com/ctuning/ctuning-programs">ctuning-programs</a> repository
to be compatible with the presented CK autotuning workflow.
We continue adding real, open-source applications and libraries as CK program entries
(GEMM, HOG, SLAM, convolutions) or manually extracting and sharing interesting code 
regions from them with the help of the community.
Such a large collection of diverse and realistic workloads 
should help make computer systems research more applied and practical.
<p>As many other scientists, we also faced a big data problem when continuously 
aggregating large amounts of raw optimization data during crowd-tuning
for further processing including machine learning&nbsp;[<a href="#ref_9">9</a>].
We managed to solve this problem in the CK by using 
online pre-processing of raw data and online classification 
to record only the most efficient optimization solutions 
(on a frontier in case of multi-objective autotuning) 
along with unexpected behavior (bugs and numerical 
instability)&nbsp;[<a href="#ref_6">6</a>].
It is now possible to invoke crowd-tuning of GCC compiler flags (improving execution time) in the CK as following:


<pre>$ ck crowdtune program --iterations=50 --scenario=8289e0cf24346aa7</pre>


<p>or
<p>

<pre>$ ck crowdsource program.optimization --iterations=50 --scenario=8289e0cf24346aa7</pre>


<p>In contrast with traditional autotuning, CK will first query <i>remote-ck</i> repository
to obtain all most efficient optimization choices aka solutions (combinations of random compiler flags in our example)
for a given trade-off scenario (GCC compiler flag tuning to minimize execution time), compiler version,
platform and OS.
CK will then select a random CK program (computational species),
compiler and run it with all these top optimizations,
and then try N extra random optimizations (random combinations of GCC flags) 
to continue increasing design and optimization space coverage.
CK will then send the highest improvements of monitored characteristics 
(execution time in our example) achieved for each optimization solution as well as worst degradations
back to a public server.
If a new optimization solution if also found during random autotuning,
CK will assign it a unique ID (<i>solution_uid</i> 
and will record it in a public repository.
At the public server side, CK will merge improvements and degradations for a given
program from a participant with a global statistics while recording how many programs 
achieved the highest improvement (best species) or worst degradation (worst species) for a given optimization
as shown in Figure&nbsp;<a href="#fig_ck_snapshot_of_results_gcc4">17</a>.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_snapshot_of_results_gcc4"><b>Figure 17</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/6fa4e9181faa1385-cropped.png" width="700"><br>
      <br>
       <table border="1" cellpadding="3" cellspacing="0">
 <tr>
  <td align="right"><b>Solution</b></td>
  <td><b>Pruned flags (complexity reduction)</b></td>
  <td><b>Best species</b></td>
  <td><b>Worst species</b></td>
 </tr>
 <tr>  <td>1</td>
  <td>-O3 -flto</td>
  <td>6</td>
  <td>3</td>
 </tr> <tr>  <td>2</td>
  <td>-O3 -fno-inline -flto</td>
  <td>1</td>
  <td>1</td>
 </tr> <tr>  <td>3</td>
  <td>-O3 -fno-if-conversion2 -funroll-loops</td>
  <td>2</td>
  <td>1</td>
 </tr> <tr>  <td>4</td>
  <td>-O3 -fpeel-loops -ftracer</td>
  <td>1</td>
  <td>3</td>
 </tr> <tr>  <td>5</td>
  <td>-O3 -floop-nest-optimize -fno-sched-interblock -fno-tree-copy-prop -funroll-all-loops</td>
  <td>4</td>
  <td>1</td>
 </tr> <tr>  <td>6</td>
  <td>-O3 -funroll-loops</td>
  <td>2</td>
  <td>3</td>
 </tr> <tr>  <td>7</td>
  <td>-O3 -floop-strip-mine -funroll-loops</td>
  <td>1</td>
  <td>1</td>
 </tr> <tr>  <td>8</td>
  <td>-O3 -fno-inline -fno-merge-all-constants -fno-tree-ccp -funroll-all-loops</td>
  <td>2</td>
  <td>3</td>
 </tr> <tr>  <td>9</td>
  <td>-O3 -fno-tree-loop-if-convert -funroll-all-loops</td>
  <td>3</td>
  <td>2</td>
 </tr> <tr>  <td>10</td>
  <td>-O3 -fno-section-anchors -fselective-scheduling2 -fno-tree-forwprop -funroll-all-loops</td>
  <td>2</td>
  <td>2</td>
 </tr> <tr>  <td>11</td>
  <td>-O3 -fno-ivopts -funroll-loops</td>
  <td>4</td>
  <td>1</td>
 </tr> <tr>  <td>12</td>
  <td>-O3 -fno-tree-ch -funroll-all-loops</td>
  <td>1</td>
  <td>1</td>
 </tr> <tr>  <td>13</td>
  <td>-O3 -fno-move-loop-invariants -fno-tree-ch -funroll-loops</td>
  <td>1</td>
  <td>2</td>
 </tr> <tr>  <td>14</td>
  <td>-O3 -fira-algorithm=priority -fno-ivopts</td>
  <td>1</td>
  <td>2</td>
 </tr> <tr>  <td>15</td>
  <td>-O3 -fno-ivopts</td>
  <td>2</td>
  <td>4</td>
 </tr> <tr>  <td>16</td>
  <td>-O3 -fno-sched-spec -fno-tree-ch</td>
  <td>1</td>
  <td>2</td>
 </tr> <tr>  <td>17</td>
  <td>-O3 -fno-ivopts -fselective-scheduling -fwhole-program</td>
  <td>1</td>
  <td>1</td>
 </tr> <tr>  <td>18</td>
  <td>-O3 -fno-omit-frame-pointer -fno-tree-loop-optimize</td>
  <td>1</td>
  <td>4</td>
 </tr> <tr>  <td>19</td>
  <td>-O3 -fno-auto-inc-dec -ffinite-math-only</td>
  <td>1</td>
  <td>2</td>
 </tr> <tr>  <td>20</td>
  <td>-O3 -fno-guess-branch-probability -fira-loop-pressure -fno-toplevel-reorder</td>
  <td>1</td>
  <td>5</td>
 </tr> <tr>  <td>21</td>
  <td>-O3 -fselective-scheduling2 -fno-tree-pre</td>
  <td>2</td>
  <td>2</td>
 </tr> <tr>  <td>22</td>
  <td>-O3 -fgcse-sm -fno-move-loop-invariants -fno-tree-forwprop -funroll-all-loops -fno-web</td>
  <td>1</td>
  <td>0</td>
 </tr> <tr>  <td>23</td>
  <td>-O3 -fno-schedule-insns -fselective-scheduling2</td>
  <td>1</td>
  <td>2</td>
 </tr></table>
<p>      <br>
      <a href="http://cknowledge.org/repo/web.php?template=cknowledge&wcid=8289e0cf24346aa7:d24a4fde9f120e10">[ Latest live results in online repository and replay info ]</a><br>
      
$#ck_access_start#$
{"action":"show","force_url":"http://cknowledge.org/repo/web.php?", "from_repo":"remote-ck","module_uoa":"experiment.tune.compiler.flags", "change_module_uoa":"8289e0cf24346aa7", "data_uoa":"d24a4fde9f120e10", "minimal":"yes", "graph_d3_div":"ck_interactive_3c4b61eebd18c00b", "remove_script_src":"yes"}
$#ck_access_stop#$
<p>     <br><i>
      Snapshot of top performing combinations of GCC 4.9.2 compiler flags together with highest speedups and worst degradations achieved across all shared CK workloads on RPi3.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_snapshot_of_results_gcc4} -->


   </div></center>

<p>
This figure shows a snapshot of public optimization results 
with top performing combinations of GCC 4.9.2 compiler flags
on RPi3 devices which minimize execution time of shared CK workloads 
(programs and data sets) in comparison with <i>-O3</i> optimization level.
It also shows the highest speedup and the worse degradation achieved
across all CK workloads for a given optimization solution, as well
as a number of workloads where this solution was the best or the worst
(online classification of all optimization solutions).
Naturally this snapshot automatically generated from the public repository 
at the time of publication may slightly differ from continuously updated 
live optimization results available at this&nbsp;<a href="http://cknowledge.org/repo/web.php?template=cknowledge&wcid=8289e0cf24346aa7:d24a4fde9f120e10">link</a>.
These results confirm that GCC 4.9.2 misses many optimization opportunities 
not covered by <i>-O3</i> optimization level.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_snapshot_of_results_gcc7"><b>Figure 18</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/41ed88477436dd3e-cropped.png" width="700"><br>
      <br>
       <table border="1" cellpadding="3" cellspacing="0">
 <tr>
  <td align="right"><b>Solution</b></td>
  <td><b>Pruned flags (complexity reduction)</b></td>
  <td><b>Best species</b></td>
  <td><b>Worst species</b></td>
 </tr>
 <tr>  <td>1</td>
  <td>-O3 -fno-delayed-branch -flto -fno-selective-scheduling2 -fno-whole-program</td>
  <td>6</td>
  <td>0</td>
 </tr> <tr>  <td>2</td>
  <td>-O3 -flto</td>
  <td>4</td>
  <td>1</td>
 </tr> <tr>  <td>3</td>
  <td>-O3 -fno-inline -flto</td>
  <td>2</td>
  <td>1</td>
 </tr> <tr>  <td>4</td>
  <td>-O3 -fno-cprop-registers -flto -funroll-all-loops</td>
  <td>3</td>
  <td>1</td>
 </tr> <tr>  <td>5</td>
  <td>-O3 -fno-tree-fre -funroll-all-loops</td>
  <td>2</td>
  <td>1</td>
 </tr> <tr>  <td>6</td>
  <td>-O3 -fno-predictive-commoning -fno-schedule-insns -funroll-loops</td>
  <td>3</td>
  <td>3</td>
 </tr> <tr>  <td>7</td>
  <td>-O3 -funroll-loops</td>
  <td>3</td>
  <td>0</td>
 </tr> <tr>  <td>8</td>
  <td>-O3 -fno-tree-ter -funroll-all-loops</td>
  <td>3</td>
  <td>1</td>
 </tr> <tr>  <td>9</td>
  <td>-O3 -fno-merge-all-constants -fselective-scheduling2 -funroll-loops</td>
  <td>1</td>
  <td>0</td>
 </tr> <tr>  <td>10</td>
  <td>-O3 -fno-devirtualize-at-ltrans -fno-predictive-commoning -fno-tree-pre</td>
  <td>1</td>
  <td>2</td>
 </tr> <tr>  <td>11</td>
  <td>-O3 -fcheck-data-deps -fira-loop-pressure -fno-isolate-erroneous-paths-dereference -fno-sched-dep-count-heuristic -fsection-anchors -fsemantic-interposition -fno-tree-ch -fno-tree-loop-linear -fno-tree-partial-pre</td>
  <td>2</td>
  <td>2</td>
 </tr> <tr>  <td>12</td>
  <td>-O3 -fno-schedule-insns -ftracer</td>
  <td>2</td>
  <td>3</td>
 </tr> <tr>  <td>13</td>
  <td>-O3 -fno-auto-inc-dec -fguess-branch-probability -fipa-pure-const -freorder-blocks -fselective-scheduling2 -ftree-ccp -fno-tree-pre -ftree-tail-merge</td>
  <td>1</td>
  <td>1</td>
 </tr></table>
<p>      <br>
      <a href="http://cknowledge.org/repo/web.php?template=cknowledge&wcid=8289e0cf24346aa7:79bca2b76876b5c6">[ Latest live results in online repository and replay info ]</a><br>
      
$#ck_access_start#$
{"action":"show","force_url":"http://cknowledge.org/repo/web.php?", "from_repo":"remote-ck","module_uoa":"experiment.tune.compiler.flags", "change_module_uoa":"8289e0cf24346aa7", "data_uoa":"79bca2b76876b5c6", "minimal":"yes", "graph_d3_div":"ck_interactive_1e94a8eaa9306f1e", "remove_script_src":"yes"}
$#ck_access_stop#$
<p>     <br><i>
      Snapshot of top performing combinations of GCC 7.1.0 compiler flags together with highest speedups and worst degradations achieved across all shared CK workloads on RPi3.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_snapshot_of_results_gcc7} -->


   </div></center>

<p>Figure&nbsp;<a href="#fig_ck_snapshot_of_results_gcc7">18</a> with optimization results 
for GCC 7.1.0 also confirms that this version was considerably improved 
in comparison with GCC 4.9.2
(latest live results are available in our public optimization repository
at this <a href="http://cknowledge.org/repo/web.php?wcid=8289e0cf24346aa7:79bca2b76876b5c6">link</a>):
there are fewer efficient optimization solutions found during crowd-tuning
14 vs 23 showing the overall improvement of the <i>-O3</i> optimization level.
<p>Nevertheless, GCC 7.1.0 still misses many optimization opportunities 
simply because our long-term experience suggests that it is infeasible 
to prepare one universal and efficient optimization heuristics 
with good multi-objective trade-offs for all continuously 
evolving programs, data sets, libraries, optimizations and platforms.
That is why we hope that our approach of combining a common workflow framework
adaptable to software and hardware changes, public repository of optimization knowledge, 
universal and collaborative autotuning across multiple hardware platforms 
(e.g.&nbsp;provided by volunteers or by HPC providers), and community involvement 
should help make optimization and testing of compilers
more automatic and sustainable&nbsp;[<a href="#ref_6">6</a>, <a href="#ref_35">35</a>].
Rather than spending considerable amount of time on writing their own autotuning and crowd-tuning
frameworks, students and researchers can quickly reuse shared workflows, 
reproduce and learn already existing optimizations, try to improve optimization heuristics, 
and validate their results by the community. 
<p>Furthermore, besides using <i>-Ox</i> compiler levels, academic and industrial users 
can immediately take advantage of various shared optimizations solutions automatically
found by volunteers for a given compiler and hardware via CK using <i>solution_uid</i> flag.
For example, users can test the most efficient combination of compiler flags 
which achieved the highest speedup for GCC 7.1.0 on RPi3 
(see "Copy CID to clipboard  for a given optimization solution at this 
<a href="http://cknowledge.org/repo/web.php?wcid=8289e0cf24346aa7:79bca2b76876b5c6">link</a>)
for their own programs using CK:
<p>
<pre>
$ ck benchmark program:{new program}
  --shared_solution_cid=27bc42ee449e880e:
  79bca2b76876b5c6-8289e0cf24346aa7-
  f49649288ab0accd
</pre>

<p>or
<p>
<pre>
$ ck benchmark program:{new program} 
  -O27bc42ee449e880e:79bca2b76876b5c6-
  8289e0cf24346aa7-f49649288ab0accd
</pre>

<p>



<a name="sec_collaborative">

<!-- CK_CUR_SECTION={6} -->

<h2>6&nbsp;&nbsp;Autotuning and crowd-tuning real workloads</h2>

<!-- CK_LABEL={sec_collaborative} -->



In this section we would like to show how we can apply universal autotuning 
and collaboratively found optimization solutions to several popular workloads
used by RPi community: <i>zlib decode, zlib encode, 
7z encode, aubio, ccrypt, gzip decode, gzip encode, minigzip decode, 
minigzip encode, rhash, sha512sum, unrar</i>.
We added the latest versions of these real programs 
to the CK describing how to compile and run them 
using CK JSON meta data:
<p>

<pre>$ ck ls ck-rpi-optimization:program:*</pre>


<p>We can now autotune any of these programs via CK as described in Section&nbsp;<a href="#sec_flag_autotuning">4</a>.
For example, the following command will autotune <i>zlib decode</i> workload
with 150 random combinations of compiler flags including parametric and architecture 
specific ones, and will record results in a local repository:
<p>

<pre>$ ck autotune program:zlib --cmd_key=decode
  --iterations=150 --repetitions=3 
  --scenario=experiment.tune.compiler.flags.gcc
  --parametric_flags --cpu_flags --base_flags 
  --record_uoa=tmp-rpi3-zlib-decode-gcc4-150bpc-rnd</pre>


<p>Figure&nbsp;<a href="#fig_autotuning_zlib_decode_gcc4">19</a> 
(<a href="http://cknowledge.org/repo/web.php?wcid=graph:3a97d1f6494f9d45&subgraph=rpi3-autotuning-zlib-decode-gcc4-interactive">link with interactive graph</a>) 
shows a manually annotated graph with the outcome of such autotuning 
when using GCC 4.9.2 compiler on RPi3 device
in terms of execution time with variation and code size.
Each blue point on this graph is related to one combination of random compiler flags.
The red line highlights the frontier of all autotuning results 
to let users trade off execution time and code size 
during multi-objective optimization.
Similar to graphs in Section&nbsp;<a href="#sec_flag_autotuning">4</a>, we also plotted points 
when using several main GCC and Clang optimization levels.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_decode_gcc4"><b>Figure 19</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/22f5af13e8e498aa-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:07969231dd8f9474&subpoint=cfcb9327f42ddabd">A1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   41.3&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   131140
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:9b2b24a80c45aa9b&subpoint=eb28149e9a71762d">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.2&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   101448
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:9195c9fa4d5d89af&subpoint=2432890269556b39">A3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   13.6&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   78116
  </td>
  <td valign="top">
   <small>   -Os
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f5489592a3a15bf3&subpoint=6236b2e4742629aa">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54272
  </td>
  <td valign="top">
   <small>   -O2 -flto -fno-tree-fre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:eca2f4aa2a3ab852&subpoint=a624bb4c10a1619f">A5</a>
   </b>  </td>
  <td valign="top">
   CLANG&nbsp;3.8.1
  </td>
  <td valign="top">
   38.5&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   132080
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:777b58443e536152&subpoint=e13b1e85e3e0c5d6">A6</a>
   </b>  </td>
  <td valign="top">
   CLANG&nbsp;3.8.1
  </td>
  <td valign="top">
   12.9&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   90076
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Results of GCC 4.9.2 random compiler flag autotuning of a zlib decode workload on RPi3
      device using CK with a highlighted frontier (trading-off execution time and code size) 
      and the best found combinations of flags on this frontier.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_decode_gcc4} -->


   </div></center>

<p>In contrast with <i>susan corners</i> workload, autotuning did not improve execution time 
of <i>zlib decode</i> over <i>-O3</i> level most likely because this algorithm is present
in many benchmarking suits. 
On the other hand, autotuning impressively improved code size over <i>-O3</i> 
by nearly 2x without sacrificing execution time, and by &nbsp;1.5x with 11&#37; execution time
improvement over <i>-Os</i> (reduced optimization solution <b>A4R</b>), 
showing that code size optimization is still a second class citizen.
<p>Since local autotuning can still be quite costly (150 iterations to achieve above results),
we can now first check 10..20 most efficient combinations of compiler flags 
already found and shared by the community for this compiler and hardware
(Figure&nbsp;<a href="#fig_ck_snapshot_of_results_gcc4">17</a>).
Note that programs from this section did not participate in crowd-tuning
to let us have a fair evaluation of the influence of shared optimizations 
on these programs similar to leave-one-out cross-validation in machine learning.
<p>Figure&nbsp;<a href="#fig_autotuning_zlib_decode_gcc4_reactions">20</a> shows "reactions" 
of <i>zlib decode</i> to these optimizations in terms of execution time and code size 
(<a href="http://cknowledge.org/repo/web.php?wcid=graph:47f0b282396776c4&subgraph=rpi3-autotuning-zlib-decode-gcc4-reactions-interactive">the online interactive graph</a>).
We can see that crowd-tuning solutions indeed cluster in a relatively small area 
close to <i>-O3</i> with one collaborative solution (<b>C1</b>) close to the 
best optimization solution found during lengthy autotuning (<b>A4R</b>) 
thus providing a good trade off between autotuning time, execution time and code size.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_decode_gcc4_reactions"><b>Figure 20</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/749a8998a2e29db5-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:9b2b24a80c45aa9b&subpoint=eb28149e9a71762d">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.2&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   101448
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f5489592a3a15bf3&subpoint=6236b2e4742629aa">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54272
  </td>
  <td valign="top">
   <small>   -O2 -flto -fno-tree-fre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:dfc49b5be33c1813&subpoint=ec6a2e99da2e3445">C1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   64184
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Speeding up GCC 4.9.2 autotuning of a zlib decode workload on RPi3 device using 
      10..20 best performing combinations of compiler flags already found and shared by the community
      during crowd-tuning.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_decode_gcc4_reactions} -->


   </div></center>

   <p>Autotuning <i>zlib decode</i> using <i>GCC 7.1.0</i> revels even more interesting results
in comparison with <i>susan corners</i> as shown in Figure&nbsp;<a href="#fig_autotuning_zlib_decode_gcc7">21</a> 
(<a href="http://cknowledge.org/repo/web.php?wcid=graph:2bf38fd88a0e3ba1&subgraph=rpi3-autotuning-zlib-decode-gcc7-interactive">the online interactive graph</a>).
While there is practically no execution time improvements when switching from <i>GCC 4.9.2</i> to <i>GCC 7.1.0</i>
on <i>-O3</i> and <i>-Os</i> optimization levels, <i>GCC 7.1.0 -O3</i> considerably degraded code size by nearly 20&#37;.
Autotuning also shows few opportunities on <i>GCC 7.1.0</i> in comparison with <i>GCC 4.9.2</i>
where the best found optimization <b>B4R</b> is worse in terms of a code size than <b>A4R</b> also by around 20&#37;.
These results highlight issues which both end-users and compiler designers face
when searching for efficient combinations of compiler flags or preparing the 
default optimization levels -Ox.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_decode_gcc7"><b>Figure 21</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/3f15a01f17d48b7b-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:9b2b24a80c45aa9b&subpoint=eb28149e9a71762d">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.2&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   101448
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f5489592a3a15bf3&subpoint=6236b2e4742629aa">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54272
  </td>
  <td valign="top">
   <small>   -O2 -flto -fno-tree-fre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:1d1b423cb2567413&subpoint=f42caae168d92907">B1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   41.3&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   128376
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:45b844dc97bc88bb&subpoint=c88f2a728405d8eb">B2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   11.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   119084
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:a1e26d8e4858adf5&subpoint=bbf4d22b4e9b34d5">B3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   13.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   74280
  </td>
  <td valign="top">
   <small>   -Os
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:b642fbace509ae5a&subpoint=cd944d398d208b53">B4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   11.9&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   78700
  </td>
  <td valign="top">
   <small>   -O2 -fno-early-inlining -fno-tree-fre
   </small>  </td>
 </tr>
</table>
<p>      <br>
     
$#ck_include_start#$
{"cid":"2d41f89bcf32d4d4:2bf38fd88a0e3ba1", "where":"div#ck_interactive_a30267d0c344fcdb", "html":"rpi3-autotuning-zlib-decode-gcc7-interactive.html", "style":"rpi3-autotuning-zlib-decode-gcc7-interactive.style", "add_div":"yes", "add_box":"yes", "box_width":700, "remove_script_src":"yes"}
$#ck_include_stop#$
<p>     <br><i>
       Results of GCC 7.1.0 random compiler flag autotuning of zlib decode on RPi3 device 
       with a highlighted frontier (trading-off execution time and code size), 
       the best combinations of flags on this frontier, and comparison with the results from GCC 4.9.2.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_decode_gcc7} -->


   </div></center>

   <p>CK crowd-tuning can assist in this case too - Figure&nbsp;<a href="#fig_autotuning_zlib_decode_gcc7_reactions">22</a>
shows reactions of <i>zlib decode</i> to the most efficient combinations of GCC 7.1.0 compiler flags
shared by the community for RPi3 
(<a href="http://cknowledge.org/repo/web.php?wcid=graph:a53089441c68c978&subgraph=rpi3-autotuning-zlib-decode-gcc7-reactions-interactive">the online interactive graph</a>).
Shared optimization solution&nbsp;<b>C2</b> achieved the same results in terms of execution time and code size
as reduced solution <b>B4R</b> found during 150 random autotuning iterations.
Furthermore, another shared optimization solution&nbsp;<b>C1</b> improved code size by &nbsp;15&#37; in comparison
with GCC 7.1.0 autotuning solution&nbsp;<b>B4R</b> and is close to the best solution GCC 4.9.2 autotuning solution&nbsp;<b>A4R</b>.
These results suggest that 150 iterations with random combinations of compiler flags 
may not be enough to find an efficient solution for <i>zlib decode</i>.
In turn, crowd-tuning can help considerably accelerate and focus such optimization space exploration.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_decode_gcc7_reactions"><b>Figure 22</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/74ba8b07475626f5-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f5489592a3a15bf3&subpoint=6236b2e4742629aa">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   12.1&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54272
  </td>
  <td valign="top">
   <small>   -O2 -flto -fno-tree-fre
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:45b844dc97bc88bb&subpoint=c88f2a728405d8eb">B2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   11.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   119084
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:b642fbace509ae5a&subpoint=cd944d398d208b53">B4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   11.9&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   78700
  </td>
  <td valign="top">
   <small>   -O2 -fno-early-inlining -fno-tree-fre
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f89c04c152682687&subpoint=9a4548dd347699b7">C1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   12.0&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   68464
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:f89c04c152682687&subpoint=b5a6ade146d9b028">C2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   11.6&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   81880
  </td>
  <td valign="top">
   <small>   -O3 -flto
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Testing reactions of zlib decode to top most efficient GCC 7.1.0 optimizations shared by the community for RPi3 devices vs GCC 4.9.2.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_decode_gcc7_reactions} -->


   </div></center>

   <p>We performed the same autotuning and crowd-tuning experiments for <i>zlib encode</i> workload
with the results shown in Figures&nbsp;<a href="#fig_autotuning_zlib_encode_gcc4">23</a>,&nbsp;<a href="#fig_autotuning_zlib_encode_gcc4_reactions">24</a>,&nbsp;<a href="#fig_autotuning_zlib_encode_gcc7">25</a>,&nbsp;<a href="#fig_autotuning_zlib_encode_gcc7_reactions">26</a>.
The results show similar trend that <i>-O3</i> optimization level of both <i>GCC 4.7.2</i> and <i>GCC 7.1.0</i> 
perform well in terms of execution time, while there is the same degradation in the code size when moving to a new compiler
(since we monitor the whole zlib binary size for both decode and encode functions).
Crowd-tuning also helped improve the code size though optimizations&nbsp;<b>A4R</b>,&nbsp;<b>B4R</b> and <b>C1</b>
are not the same as in case of <i>zlib decode</i>.
The reason is that algorithms are different and need different optimizations 
to keep execution time intact while improving code size.
Such result provides an extra motivation for function-level optimizations
already available in GCC.
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_encode_gcc4"><b>Figure 23</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/269da4a48ecdfe46-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:28ee633f4192a488&subpoint=463438f9b5e50e55">A1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   39.0&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   131140
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:21f631290c7846ee&subpoint=b3d50b1184e6ebed">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.0&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   101448
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:2bdb07edb41aabb0&subpoint=877f5d27c47b8845">A3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   16.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   78116
  </td>
  <td valign="top">
   <small>   -Os
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:85a9d07941d187e4&subpoint=c59cc63440c795a7">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54284
  </td>
  <td valign="top">
   <small>   -O2 -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:731a468f9643496a&subpoint=9ca6c418285e121a">A5</a>
   </b>  </td>
  <td valign="top">
   CLANG&nbsp;3.8.1
  </td>
  <td valign="top">
   38.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   132080
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:90dc03e42974d27a&subpoint=47ebb3b545c2e220">A6</a>
   </b>  </td>
  <td valign="top">
   CLANG&nbsp;3.8.1
  </td>
  <td valign="top">
   14.7&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   90076
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Results of GCC 4.9.2 random compiler flag autotuning of a zlib encode workload on RPi3
      device using CK with a highlighted frontier (trading-off execution time and code size) 
      and the best found combinations of flags on this frontier.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_encode_gcc4} -->


   </div></center>

<p>   <p>
      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_encode_gcc4_reactions"><b>Figure 24</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/8d7b4b7a7b8545e7-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:21f631290c7846ee&subpoint=b3d50b1184e6ebed">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.0&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   101448
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:85a9d07941d187e4&subpoint=c59cc63440c795a7">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54284
  </td>
  <td valign="top">
   <small>   -O2 -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:872541a6bf29037e&subpoint=f9fa9a3effb5c863">C1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.2&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   64184
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Accelerating GCC 4.9.2 autotuning of a zlib encode workload on RPi3 device using 
      10..20 best performing combinations of compiler flags already found 
      and shared by the community during collaborative optimization.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_encode_gcc4_reactions} -->


   </div></center>

   <p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_encode_gcc7"><b>Figure 25</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/2318dcb514171ad4-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:21f631290c7846ee&subpoint=b3d50b1184e6ebed">A2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.0&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   101448
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:85a9d07941d187e4&subpoint=c59cc63440c795a7">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54284
  </td>
  <td valign="top">
   <small>   -O2 -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:755bdc4154a3240e&subpoint=479684c44854800c">B1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   38.8&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   128376
  </td>
  <td valign="top">
   <small>   
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:50948cede943469a&subpoint=381aef856bc24d3d">B2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   13.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   119084
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:4cc78e1a736bc05e&subpoint=7177c749f6b5004a">B3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   15.9&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   74280
  </td>
  <td valign="top">
   <small>   -Os
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:3a9a0b4e4740a607&subpoint=a8e09e075b5b4a38">B4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   13.7&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   52424
  </td>
  <td valign="top">
   <small>   -O2 -fgcse-after-reload -flto -fschedule-fusion -fno-ssa-phiopt -fno-tree-fre
   </small>  </td>
 </tr>
</table>
<p>      <br>
     
$#ck_include_start#$
{"cid":"2d41f89bcf32d4d4:df35fbf4dc6b3851", "where":"div#ck_interactive_54f71b367b787a13", "html":"rpi3-autotuning-zlib-encode-gcc7-interactive.html", "style":"rpi3-autotuning-zlib-encode-gcc7-interactive.style", "add_div":"yes", "add_box":"yes", "box_width":700, "remove_script_src":"yes"}
$#ck_include_stop#$
<p>     <br><i>
       Results of GCC 7.1.0 random compiler flag autotuning of zlib encode on RPi3 device 
       with a highlighted frontier (trading-off execution time and code size), 
       the best combinations of flags on this frontier, and comparison with the results from GCC 4.9.2.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_encode_gcc7} -->


   </div></center>

   <p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_autotuning_zlib_encode_gcc7_reactions"><b>Figure 26</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/bae9e4979bd3bc4b-cropped.png" width="900"><br>
      <br>
       <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>ID</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;(sec.)</b>
  </td>
  <td valign="top">
   <b>Size&nbsp;(bytes)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:85a9d07941d187e4&subpoint=c59cc63440c795a7">A4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   14.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   54284
  </td>
  <td valign="top">
   <small>   -O2 -flto
   </small>  </td>
 </tr>
 <tr  style="color:#4f0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:50948cede943469a&subpoint=381aef856bc24d3d">B2</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   13.2&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   119084
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  style="color:#af0000;">
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:3a9a0b4e4740a607&subpoint=a8e09e075b5b4a38">B4R</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   13.7&nbsp;&plusmn;&nbsp;0.0
  </td>
  <td valign="top">
   52424
  </td>
  <td valign="top">
   <small>   -O2 -fgcse-after-reload -flto -fschedule-fusion -fno-ssa-phiopt -fno-tree-fre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:047ddbe5ef0ab588&subpoint=0b17daf9418aaa6d">C1</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   13.3&nbsp;&plusmn;&nbsp;0.1
  </td>
  <td valign="top">
   68464
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
</table>
<p>      <br>
     <br><i>
      Analyzing reactions of zlib encode to top most efficient GCC 7.1.0 optimizations shared by the community for RPi3 devices vs GCC 4.9.2.
     </i><br><br>

     <!-- CK_LABEL={fig_autotuning_zlib_encode_gcc7_reactions} -->


   </div></center>

   <p>Besides <i>zlib</i>, we applied crowd-tuning with the best found and shared optimizations 
to other RPi programs using <i>GCC 4.9.2</i> and <i>GCC 7.1.0</i>.
Table&nbsp;<a href="#fig_crowdtuning_all_rpi3_progs">3</a> shows reactions of these optimizations
with the best trade-offs for execution time and code size.
One may notice that though <i>GCC 7.1.0 -O3</i> level improves execution time
of most of the programs apart from a few exceptions, it also considerably degrades
code size in comparison with <i>GCC 4.9.2 -O3</i> level.
These results also confirm that neither <i>-O3</i> nor <i>-Os</i> on both 
<i>GCC 4.9.2</i> and <i>GCC 7.1.0</i> achieves the best trade-offs for execution
time and code size thus motivating again our collaborative and continuous optimization
approach.
<p>    
  
  <center><div style="background-color:#f0f1f2;">

<br><a name="fig_crowdtuning_all_rpi3_progs"><b>Table 3</b><br><br>

    
      <table border="1" cellpadding="5" cellspacing="0">
 <tr style="background-color:#cfcfcf;">
  <td valign="top">
   <b>Workload</b>
  </td>
  <td valign="top">
   <b>Compiler</b>
  </td>
  <td valign="top">
   <b>Time&nbsp;improvement&nbsp;over&nbsp;-O3&nbsp;(-O3&nbsp;time&nbsp;in&nbsp;brackets)</b>
  </td>
  <td valign="top">
   <b>Binary&nbsp;size&nbsp;improvement&nbsp;over&nbsp;-O3&nbsp;(-O3&nbsp;size&nbsp;in&nbsp;brackets)</b>
  </td>
  <td valign="top">
   <b>Flags</b>
  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:860174fc1b709377&subpoint=ec934e93826572d2">7z encode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.02&nbsp;<i>(5.5&nbsp;&plusmn;&nbsp;0.1)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.52&nbsp;<i>(859728)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:03af5407d14468eb&subpoint=6d4d4e8e1c6daac6">7z encode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   no&nbsp;<i>(6.0&nbsp;&plusmn;&nbsp;1.0)</i>
  </td>
  <td valign="top">
   no&nbsp;<i>(887464)</i>
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:89fd652535db438e&subpoint=24844209902c136a">ccrypt encrypt</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   no&nbsp;<i>(7.0&nbsp;&plusmn;&nbsp;2.0)</i>
  </td>
  <td valign="top">
   no&nbsp;<i>(61772)</i>
  </td>
  <td valign="top">
   <small>   -O3
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:264c8a9a86776fcb&subpoint=f8130c563953d484">ccrypt encrypt</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.16&nbsp;<i>(7.6&nbsp;&plusmn;&nbsp;0.1)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.00&nbsp;<i>(59996)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-auto-inc-dec -fguess-branch-probability -fipa-pure-const -freorder-blocks -fselective-scheduling2 -ftree-ccp -fno-tree-pre -ftree-tail-merge
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:59dbd50c4fa98b6c&subpoint=f4f3baa9717f1cee">gzip decode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.04&nbsp;<i>(4.2&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.12&nbsp;<i>(85956)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:d2044082a58f5f78&subpoint=64f3736c4460e87f">gzip decode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.04&nbsp;<i>(4.2&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.18&nbsp;<i>(90568)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:d2044082a58f5f78&subpoint=d6ae97514d86a01a">gzip decode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.08&nbsp;<i>(4.2&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.81&nbsp;<i>(90568)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-cprop-registers -flto -funroll-all-loops
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:819a33f3e0a4efc3&subpoint=d8685bf484cb1d9c">gzip encode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.98&nbsp;<i>(12.3&nbsp;&plusmn;&nbsp;0.1)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.10&nbsp;<i>(85956)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-omit-frame-pointer -fno-tree-loop-optimize
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:7bd35343c393218c&subpoint=ea66fb473741f8ba">gzip encode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.01&nbsp;<i>(12.3&nbsp;&plusmn;&nbsp;0.8)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.18&nbsp;<i>(90568)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:3308b411e99c6c19&subpoint=1970017ea1e6fd3f">minigzip decode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.24&nbsp;<i>(10.0&nbsp;&plusmn;&nbsp;4.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.60&nbsp;<i>(101432)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:3308b411e99c6c19&subpoint=704e0a8e94359933">minigzip decode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.32&nbsp;<i>(10.0&nbsp;&plusmn;&nbsp;4.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.00&nbsp;<i>(101432)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fselective-scheduling2 -fno-tree-pre
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:b9728f4c7b36ac8a&subpoint=f72e2b4f5e47b98c">minigzip decode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.14&nbsp;<i>(8.0&nbsp;&plusmn;&nbsp;3.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.76&nbsp;<i>(119088)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:8c4b2b03f4aa21a7&subpoint=243c5a4fc20ee826">minigzip encode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.89&nbsp;<i>(9.9&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.60&nbsp;<i>(101432)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:18828143558b3c2f&subpoint=0e1438a8a38e6fd6">minigzip encode</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.00&nbsp;<i>(9.6&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.76&nbsp;<i>(119088)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:7ca1118331485cb4&subpoint=4b74181efc011cbd">rhash sha3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.00&nbsp;<i>(4.8&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.12&nbsp;<i>(14848)</i>
  </td>
  <td valign="top">
   <small>   -O3 -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:94a5792a3a4b925f&subpoint=fc8e3a495ab4298c">rhash sha3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.35&nbsp;<i>(5.2&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.30&nbsp;<i>(16396)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:94a5792a3a4b925f&subpoint=dab8b6c57a392ca5">rhash sha3</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.48&nbsp;<i>(5.2&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.07&nbsp;<i>(16396)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-schedule-insns -ftracer
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:b8ab612f1524fa3b&subpoint=042129a7110f3e31">sha512sum sha512</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.12&nbsp;<i>(7.8&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.06&nbsp;<i>(125372)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-schedule-insns -fselective-scheduling2
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:20995294bf184644&subpoint=fa8f9e4af29f3a8c">sha512sum sha512</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.22&nbsp;<i>(7.3&nbsp;&plusmn;&nbsp;0.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.07&nbsp;<i>(121180)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-predictive-commoning -fno-schedule-insns -funroll-loops
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:1a89c378d17ce4d0&subpoint=981b7b9de6f50286">unrar</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.97&nbsp;<i>(18.0&nbsp;&plusmn;&nbsp;4.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.38&nbsp;<i>(326572)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:1a89c378d17ce4d0&subpoint=94451a72a5aadda2">unrar</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;4.9.2
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.13&nbsp;<i>(18.0&nbsp;&plusmn;&nbsp;4.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.80&nbsp;<i>(326572)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-section-anchors -fselective-scheduling2 -fno-tree-forwprop -funroll-all-loops
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:cbeb48dd9eaec742&subpoint=82db107f34066a74">unrar</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.96&nbsp;<i>(18.0&nbsp;&plusmn;&nbsp;6.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.38&nbsp;<i>(326572)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-inline -flto
   </small>  </td>
 </tr>
 <tr  >
  <td valign="top">
   <b>   <a href="http://cknowledge.org/repo/web.php?wcid=experiment:cbeb48dd9eaec742&subpoint=8a5a431da242467a">unrar</a>
   </b>  </td>
  <td valign="top">
   GCC&nbsp;7.1.0
  </td>
  <td valign="top">
   &nbsp;&nbsp;1.07&nbsp;<i>(18.0&nbsp;&plusmn;&nbsp;6.0)</i>
  </td>
  <td valign="top">
   &nbsp;&nbsp;0.78&nbsp;<i>(326572)</i>
  </td>
  <td valign="top">
   <small>   -O3 -fno-tree-ter -funroll-all-loops
   </small>  </td>
 </tr>
</table>
<p>    <br><i>
      The highest found improvements (degradations) in execution time and binary size 
      for several important RPi3 programs 
      as reactions to top most efficient shared optimizations 
      for GCC 4.9.2 and GCC 7.1.0.
    </i><br><br>

    <!-- CK_LABEL={fig_crowdtuning_all_rpi3_progs} -->


  </div></center>

<p>Indeed, a dozen of shared most efficient optimizations at <a href="http://cKnowledge.org/repo">cKnowledge.org/repo</a> 
is enough to either improve execution time of above programs by up to 1.5x or code size by up to 1.8x or even
improve both size and speed at the same time.
It also helps end-users find the most efficient optimization no matter which compiler, environment and hardware are used.
<p>We can also notice that 11 workloads (computational species) 
share <i>-O3 -fno-inline -flto</i> combination of flags 
to achieve the best trade-off between execution time and code size.
This result supports our original research to use workload features, 
hardware properties, crowd-tuning and  machine learning
to predict such optimizations&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_24">24</a>, <a href="#ref_6">6</a>].
However, in contrast with the past work, we are now able
to gradually collect a large, realistic (i.e. not randomly
synthesized) set of diverse workloads with the help of the
community to make machine learning statistically meaningful.
<p>All scripts to reproduce experiments from this section are available in the following CK entries:
<p>

<pre>$ ck find script:rpi3-zlib-decode*

$ ck find script:rpi3-zlib-encode*

$ ck find script:rpi3-all-autotune</pre>


<p>



<a name="sec_crowdfuzzing">

<!-- CK_CUR_SECTION={7} -->

<h2>7&nbsp;&nbsp;Crowd-fuzzing compilers</h2>

<!-- CK_LABEL={sec_crowdfuzzing} -->



When distributing compiler autotuning and learning across diverse environments, 
compilers and devices&nbsp;[<a href="#ref_5">5</a>, <a href="#ref_6">6</a>] 
we noticed that about 10..15&#37; of randomly generated combinations 
of flags can crash a compiler or produce wrong code with segmentation faults
or incorrect output.
Indeed our approach stresses various unexpected combinations 
of compiler optimizations across diverse and possibly untested platforms and workloads
thus helping automatically detect software and hardware bugs.
It complements well-known fuzzing techniques for automatic software 
testing&nbsp;[<a href="#ref_78">78</a>, <a href="#ref_79">79</a>, <a href="#ref_80">80</a>].
<p>Our CK-based customizable autotuning workflow can assist in creating, 
learning and improving such collaborative fuzzers 
which can distribute testing across diverse platforms and workloads 
provided by volunteers while sharing and reproducing bugs.
We just need to retarget our autotuning workflow to search for
bugs instead of or together with improvements in performance, 
energy, size and other characteristics.
<p>We prepared an example scenario&nbsp;<i>experiment.tune.compiler.flags.gcc.fuzz</i>
to randomly generate compiler flags for any GCC and record only cases
with failed program pipeline.
One can use it in a same way as any CK autotuning while selecting 
above scenario as following:
<p>

<pre>$ ck autotune program:cbench-automotive-susan --iterations=150 --repetitions=3 
  --scenario=experiment.tune.compiler.flags.gcc.fuzz
  --cmd_key=corners --record_uoa=tmp-susan-corners-gcc7-150-rnd-fuzz</pre>


<p>It is then possible to view all results with unexpected behavior 
in a web browser and reproduce individual cases 
on a local or different machine as following: 
<p>

<pre>$ ck browser experiment:tmp-susan-corners-gcc7-150-rnd-fuzz</pre>
<pre>$ ck replay experiment:tmp-susan-corners-gcc7-150-rnd-fuzz</pre>


<p>We performed the same auto-fuzzing experiments for <i>susan corners</i> program 
with both <i>GCC 4.9.2</i> and <i>GCC 7.1.0</i> as in Section&nbsp;<a href="#sec_flag_autotuning">4</a>.
These results are available in the following CK entries:


<pre>$ ck search experiment:rpi3-*fuzz*</pre>


It is also possible to browse them&nbsp;<a href="http://cknowledge.org/repo/web.php?wcid=experiment:rpi3-*fuzz*">online</a>.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_trivial_fuzzing_example"><b>Figure 27</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/60e96767e8bd4522-cropped.png" width="700"><br>
     <br><i>
       Basic example of reproducing and reducing GCC bugs after random compiler flag autotuning.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_trivial_fuzzing_example} -->


   </div></center>

<p>Figure&nbsp;<a href="#fig_ck_trivial_fuzzing_example">27</a> shows a simple example of reproducing 
a GCC bug using CK together with the original random combination of flags 
and the reduced one.
GCC flag <i>-fcheck-data-deps</i> compares several passes for dependency analysis
and report a bug in case of discrepancy.
Such discrepancy was automatically found when autotuning <i>susan corners</i> 
using <i>GCC 4.9.2</i> on RPi3.
<p>Since CK automatically adapts to a user environment, it is also possible
to reproduce the same bug using a different compiler version.
Compiling the same program with the same combination of flags on the same platform
using <i>GCC 7.1.0</i> showed that this bug has been fixed in the latest compiler.
<p>We hope that our extensible and portable benchmarking workflow will help students and engineers 
prototype and crowdsource different types of fuzzers.
It may also assist even existing projects&nbsp;[<a href="#ref_81">81</a>, <a href="#ref_82">82</a>]
to crowdsource fuzzing across diverse platforms and workloads.
For example, we collaborate with colleagues from Imperial College London
to develop CK-based, continuous and collaborative OpenGL and OpenCL compiler 
fuzzers&nbsp;[<a href="#ref_83">83</a>, <a href="#ref_84">84</a>, <a href="#ref_85">85</a>]
while aggregating results from users in public or private repositories 
(&nbsp;<a href="http://cknowledge.org/repo/web.php?template=cknowledge&wcid=bc0409fb61f0aa82:1b437e72c74fe782&table_sort=2">link to public OpenCL fuzzing results across diverse desktop and mobile platforms</a>).
<p><i>All scripts to reproduce experiments from this section are available in the following CK entry:</i>
<p>

<pre>$ ck find script:rpi3-susan-fuzz-bugs</pre>


<p>



<a name="sec_crowdmodeling">

<!-- CK_CUR_SECTION={8} -->

<h2>8&nbsp;&nbsp;Unifying and crowdsourcing machine learning</h2>

<!-- CK_LABEL={sec_crowdmodeling} -->



Having all optimization statistics continuously aggregated in a repository 
in a common format with JSON meta description 
makes it relatively straightforward to apply various machine learning 
and predictive analytics techniques including 
decision trees, nearest neighbor classifiers, support vector machines (SVM)
and deep learning&nbsp;[<a href="#ref_86">86</a>, <a href="#ref_87">87</a>].
These techniques can help automate detection of regularities and consistent patterns in program behavior,
build models, and predict efficient optimizations rather than
continuously re-optimizing each new program as we previously demonstrated 
in the MILEPOST project&nbsp;[<a href="#ref_24">24</a>, <a href="#ref_88">88</a>].
Furthermore, we can now teach students how to collaboratively model 
the behavior of all computer systems, speed up optimization space exploration, 
and improve predictions of the most efficient software and hardware optimizations
based on various program, data set, platform and run-time 
features&nbsp;[<a href="#ref_4">4</a>, <a href="#ref_6">6</a>].
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_crowdmodeling"><b>Figure 28</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/c5d0a4d5514e4c98-cropped.png" width="800"><br>
     <br><i>
       Universal and high-level Collective Knowledge workflow to connect various communities 
       for collaborative, continuous and semi-automatic learning of multi-objective optimizations
       using shared machine learning modules (plugins) with the unified CK API.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_crowdmodeling} -->


   </div></center>

<p>    
  
  <center><div style="background-color:#f0f1f2;">

<br><a name="fig_crowdmodeling_milepost_all_rpi3_progs"><b>Table 4</b><br><br>

    
     <table border="1">
 <tr >
  <td >
   <b>Model</b>
  </td>
  <td >
   <b>Features</b>
  </td>
  <td >
   <b>Accuracy&nbsp;(GCC&nbsp;4.9.2)</b>
  </td>
  <td >
   <b>Accuracy&nbsp;(GCC&nbsp;7.1.0)</b>
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft56
  </td>
  <td >
   0.37
  </td>
  <td >
   0.30
  </td>
 </tr>
</table>
<p>    <br><i>
     Accuracy of the nearest neighbor classifier with MILEPOST features 
     to predict the most efficient combinations of compiler flags 
     for GCC 4.9.2 and GCC 7.1.0 flags on RPi3 device.
    </i><br><br>

    <!-- CK_LABEL={fig_crowdmodeling_milepost_all_rpi3_progs} -->


  </div></center>

<p>To demonstrate our approach, we converted all our past research artifacts 
on machine learning based optimization and SW/HW co-design
to CK modules.
We then assembled them to a universal Collective Knowledge workflow 
shown in Figure&nbsp;<a href="#fig_ck_crowdmodeling">28</a>.
If you do not know about machine learning based compiler optimizations, 
we suggest that you start from our MILEPOST GCC paper&nbsp;[<a href="#ref_24">24</a>]
to make yourself familiar with terminology 
and methodology for machine learning training 
and prediction used further.
Next, we will briefly demonstrate the use of this customizable workflow 
to continuously classify shared workloads presented in this report 
in terms of the most efficient compiler optimizations
while using MILEPOST models and features.
<p>      
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_reactions_gcc4"><b>Figure 29</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/ba5ecb7074b4985a-cropped.png" width="900"><br>
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/8cb40f9e0ee52bcd-cropped.png" width="900"><br>
     <br><i>
       Top graph: reactions of all workloads 
          to all top performing combinations of optimizations
          for GCC 4.9.2 on RPi3 device (speedups if value is more than 1.0).
       Bottom graph: groups of workloads achieving the highest speedup
          for a given unique combination of optimizations.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_reactions_gcc4} -->


   </div></center>

<p>      
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_reactions_gcc7"><b>Figure 30</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/29119378e09b4dc8-cropped.png" width="900"><br>
      <br>
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/2d416955df7546b1-cropped.png" width="900"><br>
     <br><i>
       Top graph: reactions of all workloads 
          to all top performing combinations of optimizations
          for GCC 7.1.0 on RPi3 device (speedup if value is more than 1.0).
       Bottom graph: groups of workloads achieving the highest speedup
          for a given unique combination of optimizations.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_reactions_gcc7} -->


   </div></center>

<p>First, we query the public CK repository&nbsp;[<a href="#ref_38">38</a>]
to collect all optimization statistics together with all associated objects 
(workloads, data sets, platforms) for a given optimization scenario. 
In our compiler flag optimization scenario, we retrieve all
most efficient compiler flags combinations found and shared
by the community when crowd-tuning GCC 4.9.2 on RPi3 device
(Figure&nbsp;<a href="#fig_ck_snapshot_of_results_gcc4">17</a>).
<p>Note that our CK crowd-tuning workflow also continuously applies
such optimization to all shared workloads.
This allows us to analyze "reaction" of any given workload 
to all most efficient optimizations.
We can then group together those workloads which exhibit similar reactions.
<p>The top graph in Figure&nbsp;<a href="#fig_ck_reactions_gcc4">29</a> shows reactions of all workloads 
to the most efficient optimizations as a ratio of the default execution time (-O3) 
to the execution time of applied optimization.
It confirms yet again ([<a href="#ref_6">6</a>]) that there is no single "winning" 
combination of optimizations and they can either considerably improve or degrade execution time 
on different workloads.
It also confirms that it is indeed possible to group together multiple workloads 
which share the most efficient combination of compiler flags, i.e. which achieve 
the highest speedup for a common optimization as shown in the bottom graph 
in Figure&nbsp;<a href="#fig_ck_reactions_gcc4">29</a>.
Figure&nbsp;<a href="#fig_ck_reactions_gcc7">30</a> shows similar trends for GCC 7.1.0 on the same RPi3 device
even though the overall number of the most efficient combinations of compiler flags is smaller 
than for GCC 4.9.2 likely due to considerably improved internal optimization heuristics over the past years
(see Figure&nbsp;<a href="#fig_ck_snapshot_of_results_gcc7">18</a>).
<p>Having such groups of labeled objects (where labels are the most efficient optimizations
and objects are workloads) allows us to use standard machine learning classification methodology.
One must find such a set of objects' features and a model which maximizes 
correct labeling of previously unseen objects, or in our cases can correctly predict 
the most efficient software optimization and hardware design for a given workload.
As example, we extracted 56 so-called MILEPOST features described in&nbsp;[<a href="#ref_24">24</a>] 
(static program properties extracted from GCC's intermediate representation) 
from all shared programs, stored them in <i>program.static.features</i>,
and applied simple nearest neighbor classifier to above data.
We then evaluated the quality of such model (ability to predict) using prediction accuracy
during standard leave-one-out cross-validation technique: for each workload we remove it
from the training set, build a model, validate predictions, sum up all correct predictions 
and divide by the total number of workloads.
<p>Table&nbsp;<a href="#fig_crowdmodeling_milepost_all_rpi3_progs">4</a> shows this prediction accuracy
of our MILEPOST model for compiler flags from GCC 4.9.2 and GCC 7.1.0 
across all shared workloads on RPi3 device.
One may notice that it is nearly twice lower than in the
original MILEPOST
paper&nbsp;[<a href="#ref_24">24</a>].
As we explain in&nbsp;[<a href="#ref_6">6</a>],
in the MILEPOST project we could only use a dozen of similar
workloads and just a few most efficient optimizations to be
able to perform all necessary experiments within a reasonable
amount of time (6 months).
After brining the community on hoard, we could now use a much larger 
collective training set with more than 300 shared, diverse 
and non-synthesized workloads while analyzing much more optimizations 
by crowdsourcing autotuning.
This helps obtain a more realistic limit of the MILEPOST predictor.
<p>Though relatively low, this number can now become a 
reference point to be further improved by the community.
It is similar in spirit to the ImageNet Large Scale Visual Recognition Competition
(ILSVRC)&nbsp;[<a href="#ref_89">89</a>]
which reduced image classification error rate from 25&#37;
in 2011 to just a few percent with the help of the community.
Furthermore, we can also keep just a few representative
workloads for each representative group as well as misclassified ones in
a public repository thus producing a minimized, realistic and
representative training set for systems researchers.
<p><i>We shared all demo scripts which we used to generate data and
graphs in this section in the following CK entry (however they are not yet
user-friendly and we will continue improving documentation and
standardizing APIs of reusable CK modules with the help of the community):</i>
<p>

<pre>$ ck find script:rpi3-crowdmodel</pre>


<p>



<a name="sec_features">

<!-- CK_CUR_SECTION={9} -->

<h2>9&nbsp;&nbsp;Improving and autotuning models and features</h2>

<!-- CK_LABEL={sec_features} -->



There are many publications demonstrating interesting machine learning algorithms,
features and models to predict efficient program optimizations and hardware
designs&nbsp;[<a href="#ref_90">90</a>, <a href="#ref_62">62</a>, <a href="#ref_91">91</a>, <a href="#ref_92">92</a>, <a href="#ref_93">93</a>, <a href="#ref_94">94</a>, <a href="#ref_88">88</a>, <a href="#ref_95">95</a>, <a href="#ref_66">66</a>, <a href="#ref_70">70</a>, <a href="#ref_96">96</a>, <a href="#ref_97">97</a>, <a href="#ref_98">98</a>, <a href="#ref_99">99</a>].
Though all these techniques can be potentially useful, the
lack of common interfaces and meta information for artifacts
and experimental workflows makes it extremely challenging 
to compare, reuse and build upon them particularly 
in industrial projects with tough deadlines.
<p>Even artifact evaluation which we introduced at systems
conferences&nbsp;[<a href="#ref_100">100</a>] to partially solve these issues is not
yet enough because our community does not have a common, 
portable and customizable workflow framework.
Bridging this gap between machine learning and systems research
served as an additional motivation to develop Collective Knowledge
workflow framework.
Our idea is to help colleagues and students share various workloads, 
data sets, machine learning algorithms, models and feature extractors 
as plugins (CK modules) with a common API and meta description.
Plugged to a common machine learning workflow such modules
can then be applied in parallel to continuously compete 
for the most accurate predictions for a given optimization scenario.
Furthermore, the community can continue improving and autotuning models,
analyzing various combination of features, experimenting with hierarchical models, 
and pruning models to reduce their complexity across shared data sets 
to trade off prediction accuracy, speed, size and the ease of interpretation.
<p>      
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_model_crowdtuning_gcc"><b>Figure 31</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/4d19bcd3cfe164ba-cropped.png" width="800"><br>
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/d2704b9bbf2441c8-cropped.png" width="800"><br>
     <br><i>
      Accuracy of an automatically generated decision tree to predict compiler flags (GCC 4.9.2 on the top graph and GCC 7.1.0 on the bottom graph) 
      on RPi3 when autotuning the tree depth.
      Round blue dots show prediction accuracy with cross-validation while blue crosses show
      prediction accuracy without cross-validation.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_model_crowdtuning_gcc} -->


   </div></center>

<p>For a proof-of-concept of such collaborative learning approach, 
we shared a number of customizable CK modules (see&nbsp;<i>ck search module:*model*</i>)
for several popular classifiers including the nearest neighbor,
decision trees and deep learning.
These modules serve as wrappers with a common CK API for
TensorFlow, scikit-learn, R and other machine learning frameworks.
We also shared several feature extractors (see <i>ck search module:*features*</i>) 
assembling the following groups of program features
which may influence predictions:
<p>
<ul>

  
<li>
 <b> ft1 .. ft56</b> - original MILEPOST features (see&nbsp;[<a href="#ref_24">24</a>]);
  
<li>
 <b> ft57 .. ft65</b> - additional features designed and shared by our colleague, Dr. Jeremy Singer&nbsp;[<a href="#ref_101">101</a>];
  
<li>
 <b> ft66 .. ft121</b> - original MILEPOST features normalized by the total number of instructions (ft24);
</ul>

<p>We then attempted to autotune various parameters
of machine learning algorithms exposed via CK API.
Figure&nbsp;<a href="#fig_ck_model_crowdtuning_gcc">31</a>
shows an example of autotuning the depth of a decision tree
(available as customizable CK plugin) with all shared groups of features
and its impact on prediction accuracy of compiler flags using MILEPOST
features from the previous section for GCC 4.9.2 and GCC 7.1.0
on RPi3.
Blue round dots obtained using leave-one-out validation suggest 
that decision trees of depth 8 and 4 are enough 
to achieve maximum prediction accuracy of 0.4&#37; for GCC 4.9.2 
and GCC 7.1.0 respectively.
Model autotuning thus helped improve prediction accuracy in comparison 
with the original nearest neighbor classifier from the MILEPOST project. 
<p>      
   
   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_model_crowdtuning_gcc7_dt"><b>Figure 32</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/ca86d4d4daf1d84a-cropped.png" width="300"><br>
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/11a3f4efa4479adb-cropped.png" width="800"><br>
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/68deef9c2f07e734-cropped.png" width="1000"><br>
     <br><i>
      Example of automatically generated decision trees of depth 1 and 4 with leave-one-out cross-validation, 
      and 15 without cross-validation to predict GCC 7.1.0 compiler optimizations using CK modules.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_model_crowdtuning_gcc7_dt} -->


   </div></center>

<p>Figure&nbsp;<a href="#fig_ck_model_crowdtuning_gcc7_dt">32</a> shows a few examples of such automatically
generated decision trees with different depths for GCC 7.1.0 using CK.
Such trees are easy to interpret and can therefore help compiler and hardware 
developers quickly understand the most influential features and analyze
relationships between different features and the most efficient
optimizations.
For example, the above results suggest that the number of binary integer operations (ft22) 
and the number of distinct operators (ft59) can help predict optimizations 
which can considerably improve execution time of a given method over -O3.
<p>Turning off cross-validation can also help developers understand 
how well models can perform on all available workloads (in-sample data)
(red dots on Figure&nbsp;<a href="#fig_ck_model_crowdtuning_gcc">31</a>).
In our case of GCC 7.1.0, the decision tree of depth 15 shown in Figure&nbsp;<a href="#fig_ck_model_crowdtuning_gcc7_dt">32</a>)
is enough to capture all compiler optimizations for &nbsp;300 available workloads.
<p>    
  
  
  <center><div style="background-color:#f0f1f2;">

<br><a name="fig_crowdmodeling_all_rpi3_progs"><b>Table 5</b><br><br>

    
    
      <table border="1">
 <tr >
  <td >
   <b>Model</b>
  </td>
  <td >
   <b>Features</b>
  </td>
  <td >
   <b>Accuracy&nbsp;(GCC&nbsp;4.9.2)</b>
  </td>
  <td >
   <b>Accuracy&nbsp;(GCC&nbsp;7.1.0)</b>
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 1
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.26
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 2
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.26
  </td>
  <td >
   0.36
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 3
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.27
  </td>
  <td >
   0.35
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 4
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.27
  </td>
  <td >
   0.39
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 5
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.25
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 6
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.33
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 7
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.32
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 8
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.40
  </td>
  <td >
   0.36
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 9
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.38
  </td>
  <td >
   0.31
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 10
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.37
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 11
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.34
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 12
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.32
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 13
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.35
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 14
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 15
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.34
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 16
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.35
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 17
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.34
  </td>
  <td >
   0.36
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 18
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 19
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.35
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 20
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.37
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 21
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.36
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 22
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.37
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 23
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.35
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 24
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.36
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 25
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.37
  </td>
  <td >
   0.37
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 26
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.37
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 27
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.37
  </td>
  <td >
   0.36
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 28
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.34
  </td>
  <td >
   0.34
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees with cross validation; depth 29
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.35
  </td>
  <td >
   0.37
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 1
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.39
  </td>
  <td >
   0.36
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 2
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.40
  </td>
  <td >
   0.40
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 3
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.46
  </td>
  <td >
   0.41
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 4
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.49
  </td>
  <td >
   0.47
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 5
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.55
  </td>
  <td >
   0.52
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 6
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.60
  </td>
  <td >
   0.57
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 7
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.65
  </td>
  <td >
   0.61
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 8
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.69
  </td>
  <td >
   0.68
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 9
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.73
  </td>
  <td >
   0.72
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 10
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.75
  </td>
  <td >
   0.79
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 11
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.79
  </td>
  <td >
   0.83
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 12
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.83
  </td>
  <td >
   0.87
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 13
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.85
  </td>
  <td >
   0.90
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 14
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.88
  </td>
  <td >
   0.94
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 15
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.91
  </td>
  <td >
   0.95
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 16
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.94
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 17
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.95
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 18
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.97
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 19
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.97
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 20
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 21
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 22
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 23
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 24
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 25
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 26
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 27
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 28
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   decision trees without cross validation; depth 29
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.98
  </td>
  <td >
   0.96
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 1
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.68
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 2
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.64
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 3
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.61
  </td>
  <td >
   0.45
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 4
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.64
  </td>
  <td >
   0.44
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 5
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.60
  </td>
  <td >
   0.48
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 6
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.58
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 7
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.57
  </td>
  <td >
   0.43
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 8
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.55
  </td>
  <td >
   0.37
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf with cross validation; iteration 9
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.68
  </td>
  <td >
   0.44
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 1
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.72
  </td>
  <td >
   0.29
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 2
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.72
  </td>
  <td >
   0.47
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 3
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.72
  </td>
  <td >
   0.48
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 4
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.68
  </td>
  <td >
   0.62
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 5
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.70
  </td>
  <td >
   0.48
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 6
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.55
  </td>
  <td >
   0.45
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 7
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.72
  </td>
  <td >
   0.43
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 8
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.62
  </td>
  <td >
   0.32
  </td>
 </tr>
 <tr  >
  <td >
   <b>   dnn tf without cross validation; iteration 9
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.72
  </td>
  <td >
   0.53
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft121
  </td>
  <td >
   0.30
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft56
  </td>
  <td >
   0.37
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.30
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft57&nbsp;..&nbsp;ft121
  </td>
  <td >
   0.30
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft57&nbsp;..&nbsp;ft65
  </td>
  <td >
   0.30
  </td>
  <td >
   0.30
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft66&nbsp;..&nbsp;ft121
  </td>
  <td >
   0.36
  </td>
  <td >
   0.32
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft121 (normalized)
  </td>
  <td >
   0.37
  </td>
  <td >
   0.37
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft56 (normalized)
  </td>
  <td >
   0.37
  </td>
  <td >
   0.33
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft65 (normalized)
  </td>
  <td >
   0.39
  </td>
  <td >
   0.32
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft57&nbsp;..&nbsp;ft121 (normalized)
  </td>
  <td >
   0.37
  </td>
  <td >
   0.39
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft57&nbsp;..&nbsp;ft65 (normalized)
  </td>
  <td >
   0.37
  </td>
  <td >
   0.35
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn
   </b>  </td>
  <td >
   ft66&nbsp;..&nbsp;ft121 (normalized)
  </td>
  <td >
   0.38
  </td>
  <td >
   0.38
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn (reduce complexity1)
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft121 (normalized)
  </td>
  <td >
   0.45
  </td>
  <td >
   0.44
  </td>
 </tr>
 <tr  >
  <td >
   <b>   milepost nn (reduce complexity2)
   </b>  </td>
  <td >
   ft1&nbsp;..&nbsp;ft121 (normalized)
  </td>
  <td >
   0.45
  </td>
  <td >
   0.40
  </td>
 </tr>
</table>
<p>    
    <br><i>
     Prediction accuracy when autotuning or reducing complexity of decision tree, 
     nearest neighbor and deep learning classifiers
     across different groups of program features.
    </i><br><br>

    <!-- CK_LABEL={fig_crowdmodeling_all_rpi3_progs} -->


  </div></center>

<p>To complete our demonstration of CK concepts for collaborative machine learning and optimization,
we also evaluated a deep learning based classifier from TensorFlow&nbsp;[<a href="#ref_102">102</a>]
(see <i>ck help module:model.tf</i>)
with 4 random configurations of hidden layers ([10,20,10], [21,13,21], [11,30,18,20,13], [17]) 
and training steps (300..3000).
We also evaluated the nearest neighbor classifier used in the MILEPOST project but with different groups of features 
and aggregated all results in Table&nbsp;<a href="#fig_crowdmodeling_all_rpi3_progs">5</a>. 
Finally, we automatically reduced the complexity of the nearest neighbor classifier (1) by iteratively removing those features one by one
which do not degrade prediction accuracy and (2) by iteratively adding features one by one to maximize prediction accuracy.
It is interesting to note that our nearest neighbor classifier achieves
a slightly better prediction accuracy with a reduced feature set than with 
a full set of features showing inequality of MILEPOST features
and overfitting.
<p>As expected, deep learning classification achieves a better prediction accuracy of 0.68&#37;
and 0.45&#37; for GCC 4.9.2 and GCC 7.1.0 respectively for RPi3 among currently shared models, 
features, workloads and optimizations.
However, since deep learning models are so much more computationally intensive, resource hungry
and difficult to interpret than decision trees, one must carefully balance accuracy vs speed vs size.
That is why we suggest to use hierarchical models where
high-level and coarse-grain program behavior is quickly captured 
using decision trees, while all fine-grain behavior is captured 
by deep learning and similar techniques.
Another possible use of deep learning can be in automatically capturing
influential features from the source code, data sets and hardware.
<p><i>All scripts to generate above experiments (require further documentation)
are available in the following CK entry:</i>
<p>

<pre>$ ck find script:rpi3-crowdmodel</pre>


<p>



<a name="sec_datasets">

<!-- CK_CUR_SECTION={10} -->

<h2>10&nbsp;&nbsp;Enabling input-aware optimization</h2>

<!-- CK_LABEL={sec_datasets} -->



Current prediction accuracy which we achieved for the most efficient compiler flags 
is still disappointing: around 0.45&#37; for GCC 7.1.0.
We explained this in more detail in&nbsp;[<a href="#ref_6">6</a>, <a href="#ref_4">4</a>]
by missing features particularly available at run-time from data sets and hardware.
Having a customizable experimental workflow with pluggable artifacts 
makes it relatively straightforward to analyze reactions of a given program
to the most efficient optimization across multiple data sets
and search for missing features.
<p>First, we converted 474 different data sets from the MiDataSet suite&nbsp;[<a href="#ref_103">103</a>]
as pluggable CK artifacts and shared them as a zip archive (&nbsp;800MB).
It is possible to download it from the Google Drive
from&nbsp;<a href="https://drive.google.com/open?id=0B-wXENVfIO82OUpZdWIzckhlRk0">https://drive.google.com/open?id=0B-wXENVfIO82OUpZdWIzckhlRk0</a>
(we plan to move it to a permanent repository in the future)
and then install via CK as following:
<p>

<pre>$ ck add repo --zip=ckr-ctuning-datasets.zip --quiet</pre>

<pre>$ ck ls dataset --all</pre>

<pre>$ ck search dataset --tags=image,jpeg</pre>



<p>All these data sets will be immediately visible to all related programs
via the CK autotuning workflow.
For example, if we now run <i>susan corners</i> program, CK will prompt user
a choice of 20 related images from the above data sets:
<p>

<pre>$ ck compile program:cbench-automotive-susan --speed</pre>

<pre>$ ck run program:cbench-automotive-susan</pre>



<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_datasets_jpeg_d_reactions"><b>Figure 33</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/9f93e8520adbea97-cropped.png" width="800"><br>
     <br><i>
       Reactions of a jpeg decoder across 20 distinct data sets (jpeg images)
       to all top performing combinations of compiler optimizations
       for GCC 7.1.0 on RPi3 device (speedups if value is more than 1.0).
     </i><br><br>
                                         
     <!-- CK_LABEL={fig_ck_datasets_jpeg_d_reactions} -->


   </div></center>

<p>Next, we can apply all most efficient compiler optimizations 
to a given program with all data sets.
Figure&nbsp;<a href="#fig_ck_datasets_jpeg_d_reactions">33</a> shows such reactions 
(ratio of an execution time with a given optimization to an execution time 
with the default -O3 compiler optimization) of a jpeg decoder across 
20 different jpeg images from the above MiDataSet on RPi3.
<p>One can observe that the same combination of compiler flags can both
considerably improve or degrade execution time for the same program
but across different data sets.
For example, data sets 4,5,13,16 and 17 can benefit from the most
efficient combination of compiler flags found by the community
with speedups ranging from 1.2 to 1.7.
On the other hand, it's better to run all other data sets with
the default -O3 optimization level.
<p>Unfortunately, finding data set and other features which could easily differentiate
above optimizations is often very challenging.
Even deep learning may not help if a feature is not yet exposed.
We explain this issue in&nbsp;[<a href="#ref_6">6</a>]
when optimizing real B&W filter kernel - we managed to improve
predictions by exposing a "time of the day" feature
only via human intervention.
However, yet again, the CK concept is to bring the interdisciplinary
community on board to share such cases in a reproducible way 
and then collaboratively find various features to improve predictions.
<p>Another aspect which can influence the quality of predictive models,
is that the same combinations of compiler flags are too coarse-grain
and can make different internal optimization decisions 
for different programs.
Therefore, we need to have an access to fine-grain optimizations
(inlining, tiling, unrolling, vectorization, prefetching, etc)
and related features to continue improving our models.
However, this follows our top-down optimization and modeling methodology 
which we implemented in the Collective Knowledge framework.
We want first to analyze, optimize and model coarse-grain behavior of shared workloads
together with the community and students while gradually adding more workloads, 
data sets, models and platforms.
Only when we reached the limit of prediction accuracy, 
we start gradually exposing finer-grain optimizations 
and features via extensible CK JSON interface 
while avoiding explosion in design and optimization spaces
(see details in&nbsp;[<a href="#ref_4">4</a>] for our previous
version of the workflow framework, Collective Mind).
This is much in spirit of how physicists moved from Newton's 
three coarse-grain laws of motion to fine-grain quantum mechanics.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_datasets_input_aware_autotuning"><b>Figure 34</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/37a8224467be599e-cropped.png" width="800"><br>
     <br><i>
      Performance of a tiled matrix multiply in GFLOPS for different square matrix sizes. 
      Blue circles show performance of original (non-blocked) matrix multiply
      while red crosses show best performance found during autotuning on RPi3 device.
     </i><br><br>
                                         
     <!-- CK_LABEL={fig_ck_datasets_input_aware_autotuning} -->


   </div></center>

<p>To demonstrate this approach, we shared a simple skeletonized 
matrix multiply kernel from&nbsp;[<a href="#ref_104">104</a>] in the CK format 
with blocking (tiling) parameter and data set feature 
(square matrix size) exposed via CK API:
<p>

<pre>$ ck compile program:shared-matmul-c2 --flags="-DUSE_BLOCKED_MATMUL=YES</pre>

<pre>$ ck run program:shared-matmul-c2 --env.CT_MATRIX_DIMENSION=128 --env.CT_BLOCK_SIZE=16</pre>



<p>We can then reuse universal autotuning (exploration) strategies
available as CK modules or implement specialized ones to explore 
exposed fine-grain optimizations versus different data sets.
Figure&nbsp;<a href="#fig_ck_datasets_input_aware_autotuning">34</a> shows matmul performance
in GFLOPS during random exploration of a blocking parameter for different square 
matrix sizes on RPi3.
These results are in line with multiple past studies showing that
unblocked matmul is more efficient for small matrix sizes (less than 32
on RPi3) since all data fits cache, or between 32 and 512 (on RPi3) 
if they are not power of 2.
In contrast, the tiled matmul is better on RPi3 for matrix sizes of power of 2 between 32 and 512,
since it can help reduce cache conflict misses, and for all matrix sizes more than 512
where tiling can help optimize access to slow main memory.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_ck_adaptive_systems"><b>Figure 35</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/c6d665f41a28e209-cropped.png" width="500"><br>
     <br><i>
      Enabling adaptive and self-optimizing libraries assembled from the most efficient routines
      continuously optimized by the community across different platforms and data sets. 
      These routines are selected automatically at run-time based on platform, data set and other features.
     </i><br><br>

     <!-- CK_LABEL={fig_ck_adaptive_systems} -->


   </div></center>

<p>Our customizable workflow can help teach students how to build efficient,
adaptive and self-optimizing libraries including BLAS, neural networks and FFT.
Such libraries are assembled from the most efficient routines
found during continuous crowd-tuning across numerous data sets and platforms,
and combined with fast and automatically generated decision trees  
or other more precise classifiers&nbsp;[<a href="#ref_105">105</a>, <a href="#ref_106">106</a>, <a href="#ref_107">107</a>, <a href="#ref_6">6</a>].
The most efficient routines are then selected at run-time 
depending on data set, hardware and other features as conceptually shown
in Figure&nbsp;<a href="#fig_ck_adaptive_systems">35</a>..
<p><i>All demo scripts to generate data and graphs in this section are available in the following CK entries:</i>


<pre>$ ck find script:rpi3-all-autotune-multiple-datasets</pre>

<pre>$ ck find script:rpi3-input-aware-autotune-blas</pre>



<p>



<a name="sec_competitions">

<!-- CK_CUR_SECTION={11} -->

<h2>11&nbsp;&nbsp;Reinventing computer engineering via reproducible competitions</h2>

<!-- CK_LABEL={sec_competitions} -->



Having a common and customizable workflow framework with "plug&play" artifacts 
opens up another interesting opportunity for computer engineering.
Researchers can use it to compare and improve their techniques
(optimizations, models, algorithms, architectures) 
against each other via open and reproducible competitions
while being on the same page.
<p>This is in spirit with existing machine learning competitions
such as Kaggle and ImageNet challenge&nbsp;[<a href="#ref_108">108</a>, <a href="#ref_109">109</a>] 
to improve prediction accuracy of various models.
The main difference is that we want to focus on 
optimizing the whole software/hardware/model stack 
while trading off multiple metrics including speed, accuracy,
and costs&nbsp;[<a href="#ref_40">40</a>, <a href="#ref_6">6</a>, <a href="#ref_110">110</a>].
<p>Experimental results from such competitions can be continuously aggregated 
and presented in the live Collective Knowledge scoreboard&nbsp;[<a href="#ref_38">38</a>].
Other academic and industrial researchers can then pay 
a specific attention to the "winning" techniques close 
to a Pareto frontier in a multi-dimensional 
space of accuracy, execution time, power/energy consumption, 
hardware/code/model footprint, monetary costs etc
thus speeding up technology transfer.
Furthermore, "winning" artifacts and workflows can now be recompiled,
reused and extended on the newer platforms with the latest
environment thus improving overall research sustainability.
<p>      
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_converting_ad_hoc_slambench_to_ck"><b>Figure 36</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/37f1624d4b8b53d6-cropped.png" width="800"><br>
     <br><i>
      Random exploration of various SLAM algorithms and their parameters (Simultaneous localization and mapping)
      in terms of accuracy (average trajectory error or ATE) versus speed 
      (frames per second) on RPi3 using CK.
     </i><br><br>
                                         
     <!-- CK_LABEL={fig_converting_ad_hoc_slambench_to_ck} -->


   </div></center>

<p>   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_reproducibility"><b>Figure 37</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/994e7359d7760ab1-cropped.png" width="700"><br>
     <br><i>
       Developing common evaluation methodology for empirical results in systems research:
       (a) Calculating speedups between two optimizations T1 and T2 using min, mean and expected values, and reporting max difference.
       (b) Reporting a problem when several system states are detected
     </i><br><br>

     <!-- CK_LABEL={fig_reproducibility} -->


   </div></center>

<p>For a proof-of-concept, we started helping some authors convert their 
artifacts and experimental workflows to the CK format during 
Artifact Evaluation&nbsp;[<a href="#ref_111">111</a>, <a href="#ref_55">55</a>, <a href="#ref_7">7</a>].
Association for Computing Machinery (ACM)&nbsp;[<a href="#ref_112">112</a>] 
also recently joined this effort funded by the Alfred P. Sloan Foundation 
to convert already published experimental workflows 
and artifacs from the ACM Digital Library 
to the CK format&nbsp;[<a href="#ref_113">113</a>].
<p>We can then reuse CK functionality to crowdsource benchmarking and multi-objective
autotuning of shared workloads across diverse data sets, models and platforms.
For example, Figure&nbsp;<a href="#fig_converting_ad_hoc_slambench_to_ck">36</a> shows 
results from random exploration of various SLAM algorithms (Simultaneous localization and mapping)
and their parameters from&nbsp;[<a href="#ref_114">114</a>] in terms of accuracy (average trajectory error or ATE)
versus speed (frames per second) on RPi3 using CK&nbsp;[<a href="#ref_115">115</a>]. 
Researchers may easily spend 50&#37; of their time developing
experimental, benchmarking and autotuning infrastructure 
in such complex projects, and then continuously
updating it to adapt to ever changing software and hardware
instead of innovating.
Worse, such ad-hoc infrastructure may not even survive 
the end of the project or if leading developers leave project.
<p>Using common and portable workflow framework can relieve researchers
from this burden and let them reuse already existing artifacts and
focus on innovation rather than re-developing ad-hoc software from scratch.
Other researchers can also pick up the winning designs on a Pareto frontier,
reproduce results via CK, try them on different platforms
and with different data sets, build upon them, 
and eventually try to develop more efficient algorithms.
Finally, researchers can implement a common experimental methodology 
to evaluate empirical results in systems research similar to physics 
within a common workflow framework rather than writing their own
ad-hoc scripts.
Figure&nbsp;<a href="#fig_reproducibility">37</a> shows statistical analysis 
of experimental results implemented in the CK to compare different
optimizations depending on research scenarios.
For example, we report minimal execution time from multiple experiments
to understand the limits of a given architecture, expected value to see
how a given workload performs on average, and max time to detect
abnormal behavior.
If more than one expected value is detected, it usually means
that system was in several different run-time states during experiments
(often related to adaptive changes in CPU and GPU frequency due to DVFS)
and extra analysis is required.
<p>

<center><div style="background-color:#f0f1f2;">

<br><a name="fig_reproducible_tournaments"><b>Figure 38</b>

  
    <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/b4b07ad3a7839327-cropped.png" width="600"><br>
  <br><i>
    Collective Knowledge framework as an open platform to support software/hardware/model
    co-design tournaments for Pareto-efficient deep learning
    and other emerging workloads in terms of speed, accuracy, 
    energy and various costs.
  </i><br><br>

  <!-- CK_LABEL={fig_reproducible_tournaments} -->


</div></center>

<p>We now plan to validate our Collective Knowledge approach
in the 1st reproducible ReQuEST tournament 
at the ACM ASPLOS'18 conference&nbsp;[<a href="#ref_40">40</a>]
as presented in Figure&nbsp;<a href="#fig_reproducible_tournaments">38</a>.
ReQuEST is aimed at providing a scalable tournament framework, 
a common experimental methodology and an open repository for continuous evaluation 
and optimization of the quality vs. efficiency Pareto optimality of a wide range 
of real-world applications, libraries, and models across the whole 
hardware/software stack on complete platforms. 
ReQuEST also promote reproducibility of experimental results and reusability/customization 
of systems research artifacts by standardizing evaluation methodologies and facilitating 
the deployment of efficient solutions on heterogeneous platforms. 
<p>   
   
<center><div style="background-color:#f0f1f2;">

<br><a name="fig_dnn_crowdtuning_example"><b>Figure 39</b>

     
       <br><br><img src="$#ck_root_url#$action=pull&common_func=yes&cid=1e348bd6ab43ce8a:d2af6e443fa996c6&filename=ck-assets/32c0b60dcc114aa8-cropped.png" width="800"><br>
     
$#ck_include_start#$
{"cid":"2d41f89bcf32d4d4:073216ac9e4b4ebc", "where":"div#ck_interactive_f908ef94852bfc65", "html":"interactive.html", "style":"interactive.style", "add_div":"yes", "add_box":"yes", "box_width":800, "remove_script_src":"yes", "remove_script_src":"no"}
$#ck_include_stop#$
<p>     <br><i>
      An example of a live Collective Knowledge scoreboard to crowd-benchmark
      inference in terms of speed, accuracy and platform cost 
      across diverse deep learning frameworks, models, data sets, and 
      Android devices provided by volunteers. Red dots 
      are associated with the winning workflows (model/software/hardware)
      on different frontiers.
     </i><br><br>

     <br>
     <!-- CK_LABEL={fig_dnn_crowdtuning_example} -->


   </div></center>

<p>ReQuEST will use CK and our artifact evaluation methodology&nbsp;[<a href="#ref_7">7</a>] 
to provide unified evaluation and a live scoreboard of submissions. 
Figure&nbsp;<a href="#fig_dnn_crowdtuning_example">39</a> shows a proof-of-concept example of such a
scoreboard powered by CK to collaboratively benchmark inference (speed vs. platform cost) 
across diverse deep learning frameworks (TensorFlow, Caffe, MXNet, etc.), 
models (AlexNet, GoogleNet, SqueezeNet, ResNet, etc.), real user data sets, and mobile devices 
provided by volunteers (see the latest results at <a href="http://cknowledge.org/repo">cKnowledge.org/repo</a>).
Our goal is to teach students and researchers how to 

<ul>

  
<li>
 release research artifacts of their on-going or accomplished research
as portable and reusable components, standardize evaluation workflows, 
and facilitate deployment and tech transfer of state-of-the-art research,
  
<li>
 continuously optimize various algorithms
across diverse models, data sets and platforms in terms of speed, accuracy,
size, energy usage and other costs,
  
<li>
 build upon each others' work to develop the next generation 
of efficient software and hardware stack for emerging workloads.
</ul>

<p>
<p>


<a name="sec_conclusions">

<!-- CK_CUR_SECTION={12} -->

<h2>12&nbsp;&nbsp;Conclusions and Future Work</h2>

<!-- CK_LABEL={sec_conclusions} -->



Researchers are now in a race to bring artificial intelligence to all possible
devices from IoT to supercomputers which will require 
much more efficient software and hardware then currently available.
At the same time, computer engineers have already been struggling for many years 
to develop efficient sub-components of computer systems including
algorithms, compilers and run-time systems.
<p>The major issues including raising complexity, lack of a common experimental framework 
and lack of practical knowledge exchange between academia and industry.
Rather than innovating, researchers have to spend more and more time 
writing their own, ad-hoc and not easily customizable support tools 
to perform experiments such as multi-objective autotuning.
<p>
We presented our long-term educational initiative to teach
students and researchers how to solve the above problems 
using customizable workflow frameworks similar to other sciences.
We showed how to convert ad-hoc, multi-objective
and multi-dimensional autotuning into a portable and customizable workflow 
based on open-source Collective Knowledge workflow framework.
We then demonstrated how to use it to implement various scenarios
such as compiler flag autotuning of benchmarks and realistic workloads
across Raspberry Pi 3 devices in terms of speed and size.
We also demonstrated how to crowdsource such autotuning across different
devices provided by volunteers similar to SETI@home, collect the most efficient optimizations
in a reproducible way in a public repository of knowledge at &nbsp;<a href="http://cknowledge.org/repo">cKnowledge.org/repo</a>, 
apply various machine learning techniques including decision trees, the nearest neighbor classifier
and deep learning to predict the most efficient optimizations for previously
unseen workloads, and then continue improving models and features
as a community effort.
We now plan to develop an open web platform together with the community
to provide a user-friendly front-end to all presented workflows 
while hiding all complexity.
<p>We use our methodology and open-source CK workflow framework and repository
to teach students how to exchange their research artifacts and results 
as reusable components with a a unified API and meta-information,
perform collaborative experiments, automate Artifact Evaluation
at journals and conferences&nbsp;[<a href="#ref_7">7</a>], build upon each others' work,
make their research more reproducible and sustainable, 
and eventually accelerate transfer of their ideas to industry.
Students and researchers can later use such skills and unified artifacts
to participate in our open ReQuEST tournaments on reproducible and Pareto-efficient
co-design of the whole software and hardware stack for emerging workloads
such as deep learning and quantum computing in terms of
speed, accuracy, energy and costs&nbsp;[<a href="#ref_40">40</a>].
This, in turn, should help the community build an open repository of 
portable, reusable and customizable algorithms continuously optimized
across diverse platforms, models and data sets
to assemble efficient computer systems
and accelerate innovation.
<p>



<a name="sec_ack">

<!-- CK_CUR_SECTION={13} -->

<h2>13&nbsp;&nbsp;Acknowledgments</h2>

<!-- CK_LABEL={sec_ack} -->


<p>We would like to thank Raspberry Pi foundation for initial financial support.
We are also grateful to dividiti and cTuning foundation colleagues, 
Flavio Vella, Marco Cianfriglia, Nikolay Chunosov, Daniil Efremov, Yuriy Kashnikov, 
Peter Green, Thierry Moreau and ReQuEST colleagues, and the Collective Knowledge community 
for evaluating Collective Knowledge concepts and providing useful feedback.
<p><a href="http://dividiti.com"><img src="http://cknowledge.org/images/logo-dividiti2.png" ></a>
<a href="http://cTuning.org"><img src="http://cknowledge.org/images/CTuning_foundation_logo2.png" ></a>
<a href="https://www.raspberrypi.org"><img src="http://cknowledge.org/images/logo-rpi2.png" ></a>
<p>


<a name="artifact_appendix">

<!-- CK_CUR_SECTION={A} -->

<h2>A&nbsp;&nbsp;Artifact Appendix</h2>

<!-- CK_LABEL={artifact_appendix} -->


<p><b>Submission guidelines:</b><center>
<i> <a href="http://ctuning.org/ae/submission-20161020.html">cTuning.org/ae/submission-20161020.html</a></i></center><br>

<p>This is an example of an Artifact Appendix which we introduced 
at the computer systems conferences including CGO, PPoPP, PACT and SuperComputing
to gradually unify artifact evaluation, sharing and reuse&nbsp;[<a href="#ref_7">7</a>, <a href="#ref_8">8</a>, <a href="#ref_9">9</a>, <a href="#ref_5">5</a>].
We briefly describe how to install and use our autotuning workflow, visualize optimization results and reproduce them.
We also shared all scripts which we used to generate data and
graphs in all sections from this report though we did not yet 
have time to thoroughly document them.
In fact, we plan to gradually document them and standardize APIs 
of shared CK modules with the help of the community and motivated students.
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSECTION={1} -->

<h3>A.1&nbsp;&nbsp;Abstract</h3>

<p>We provided the whole Collective Knowledge workflow with all dependencies
for collaborative, customizable, multi-dimensional and multi-objective autotuning 
of realistic workloads on Raspberry Pi 3 and other devices.
<p>Current optimization results are available for GCC 7.1.0 (<a href="http://cknowledge.org/repo/web.php?wcid=8289e0cf24346aa7:79bca2b76876b5c6">link</a>)
and for GCC 4.9.2 (<a href="http://cknowledge.org/repo/web.php?wcid=8289e0cf24346aa7:d24a4fde9f120e10">link</a>).
They are also available as a <a href="https://github.com/ctuning/ck-rpi-optimization-results">CK repository</a> 
and can be replayed on another platform via CK.
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSECTION={2} -->

<h3>A.2&nbsp;&nbsp;Description</h3>

<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSUBSECTION={1} -->

<h4>A.2.1&nbsp;&nbsp;Check-list (artifact meta information)</h4>

<p>

<ul>

  
<li>
 <b> Algorithm:</b> -
  
<li>
 <b> Program:</b> shared programs from the <a href="https://github.com/ctuning/ctuning-programs">CK ctuning-programs repository</a>
  
<li>
 <b> Compilation:</b> any GCC
  
<li>
 <b> Transformations:</b> compiler flag optimizations
  
<li>
 <b> Binary:</b> will be produced during autotuning
  
<li>
 <b> Data set:</b> real inputs from the <a href="https://github.com/ctuning/ctuning-datasets-min">CK ctuning-datasets-min repository</a>
  
<li>
 <b> Run-time environment:</b> Raspbian (or any other)
  
<li>
 <b> Hardware:</b> Raspberry Pi 3 (or any other)
  
<li>
 <b> Run-time state:</b> will be monitored by CK (CPU frequency)
  
<li>
 <b> Execution:</b> empirical measurements of the execution time of autotuned workloads via CK workflow
  
<li>
 <b> Output:</b> best combinations of GCC compiler flags that improve execution time and code size 
  
<li>
 <b> Experiment workflow:</b> autotuning, crowd-tuning  and collaborative machine learning workflow implemented using CK framework
  
<li>
 <b> Experiment customization:</b> standard customization via CK API: select compiler, programs and data sets for autotuning, crowd-tuning and predictive modeling
  
<li>
 <b> Publicly available?:</b> yes - CK autotuning and machine learning workflow (available under BSD 3-clause license) and all related artifacts are shared as reusable and customizable components via GitHub. 
</ul>


<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSUBSECTION={2} -->

<h4>A.2.2&nbsp;&nbsp;How software can be obtained (if available)</h4>

<p>You can obtain CK repositories with optimization results, shared programs and data sets, workflow for autotuning and crowd-tuning as following:
<p>

<pre>$ sudo pip install ck 

$ ck pull repo:ck-rpi-optimization-results</pre>


<p>Note that you may need around 1GB of free space. You can install 2 additional CK repositories&nbsp;[<a href="#ref_116">116</a>] 
from the public FigShare repository as following (need &nbsp;3GB of free space):
<p>

<pre>$ ck add repo:ck-rpi-optimization-results-reactions --zip=https://ndownloader.figshare.com/files/10218435 --quiet 

$ ck add repo:ck-rpi-optimization-results-reactions2 --zip=https://ndownloader.figshare.com/files/10218441 --quiet 

$ ck ls experiment:rpi3-* 
</pre>


<p>These repositories are so large because they contain all experiments from this report in a reproducible way
(we also plan to considerably reduce this size by removing duplicate information in the future). 
But if you want to prepare and run your own repositories you will likely need less than 100MB. 
See this artifact in the CK from the ACM CGO'17 paper&nbsp;[<a href="#ref_55">55</a>] as example: 
<a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">github.com/SamAinsworth/reproduce-cgo2017-paper</a>
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSUBSECTION={3} -->

<h4>A.2.3&nbsp;&nbsp;Hardware dependencies</h4>

<p>Tested on Raspberry Pi Model B 3 devices with 4-core BCM2709 processor but should work on any platform.
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSUBSECTION={4} -->

<h4>A.2.4&nbsp;&nbsp;Software dependencies</h4>

<p>
<ul>

  
<li>
 Raspbian GNU/Linux 8 (jessie)
  
<li>
 Collective Knowledge Framework&nbsp;[<a href="#ref_34">34</a>, <a href="#ref_35">35</a>]
  
<li>
 Python 2.7+ or 3.4+
  
<li>
 Git client
  
<li>
 GCC 4.9.2 or GCC 7.1.0
</ul>

<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSUBSECTION={5} -->

<h4>A.2.5&nbsp;&nbsp;Data sets</h4>

<p>A minimal set of inputs for cTuning benchmarks available from the <a href="https://github.com/ctuning/ctuning-datasets-min">CK ctuning-datasets-min repository</a>.
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSECTION={3} -->

<h3>A.3&nbsp;&nbsp;Installation</h3>

<p>Installation is performed using CK with the help of integrated cross-platform package manager&nbsp;[<a href="#ref_52">52</a>]:
<p>

<pre>$ sudo pip install ck 

$ ck pull repo:ck-rpi-optimization-results 

$ ck compile program:zlib --speed 
</pre>


<p>CK will automatically detect required software which is already installed on your platform, 
install missing packages, and prepare autotuning workflow for execution.
<p>Note that CK allows multiple versions of different software to natively co-exist.
Therefore, you can install several versions of GCC which will be automatically
detected by CK and their environment prepared accordingly.
For example, you can install (build) GCC 7.1.0 on RPi 3 via CK as following:
<p>

<pre>$ ck pull repo:ck-dev-compilers 

$ ck install package:compiler-gcc-any-src-linux-no-deps --env.PARALLEL_BUILDS=1 --env.GCC_COMPILE_CFLAGS=-O0 --env.GCC_COMPILE_CXXFLAGS=-O0 --env.EXTRA_CFG_GCC=--disable-bootstrap --env.RPI3=YES --force_version=7.1.0 

$ ck show env --tags=gcc</pre>


<p>Note that you may need to install extra dependencies including
<p>

<pre>$ sudo apt-get install texinfo build-essential libgmp-dev libmpfr-dev libisl-dev libcloog-isl-dev libmpc-dev</pre>


<p>You may also want to increase a swap size on RPi 3 to speed up GCC building. 
You can change "CONF_SWAPSIZE=100" in /etc/dphys-swapfile to "CONF_SWAPSIZE=1000". 
But do not forget to change it back after successful build to avoid damaging your SD card.
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSECTION={4} -->

<h3>A.4&nbsp;&nbsp;Experiment workflow</h3>

<p><b>Autotuning example</b>
<p>You can run zlib autotuning via CK as following:
<p>

<pre>$ ck autotune program:zlib --iterations=150 --repetitions=3 --scenario=9d88674c45b94971 --cmd_key=decode --record_uoa=my-first-experiment</pre>


<p>CK will automatically detect available compilers, will ask user to select data set, 
and will evaluate 150 combinations of random compiler flags 
(repeating each experiment 3 times for statistical analysis 
of empirical variation of results).
<p>Experimental results will be aggregated in a CK entry "experiment:my-first-experiment" a local CK repository:
<p>

<pre>$ ck find experiment:my-first-experiment</pre>


<p>You can plot graph (execution time vs binary size) or view results in a web browser as following:
<p>

<pre>$ ck plot graph:my-first-experiment 

$ ck browser experiment:my-first-experiment</pre>


<p>You can compile and run zlib program via CK as following:


<pre>$ ck compile program:zlib --flags="some flags" 

$ ck run program:zlib</pre>


<p>Finally, you can participate in GCC crowd-tuning as following:


<pre>$ ck crowdsource program.optimization --gcc</pre>


<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSECTION={5} -->

<h3>A.5&nbsp;&nbsp;Evaluation and expected result</h3>

<p>You can find all scripts to perform experiments from this article as following:
<p>

<pre>$ ck ls ck-rpi-optimization-results:script:* | sort</pre>


<p>You can then go to each individual entry and see related scripts:


<pre>$ ls `ck find script:rpi3-susan-autotune`</pre>


<p>You can find all experimental results in the following entries:


<pre>$ ck ls ck-rpi-optimization-results:experiment:* | sort</pre>


<p>You can then browse all results in your web browser as following:


<pre>$ ck browser experiment:rpi3-zlib-decode-gcc4-150b-rnd-frontier</pre>


<p>You can find information about how to replay each autotuning iteration there, for example:


<pre>$ ck replay experiment:b0f31c56475aa510 --point=46049203405c5347</pre>


<p>CK should normally show expected and new results while reporting any unexpected 
behavior (if difference is more than some threshold such as 5&#37;).
<p>


<a name="experimental_methodology">

<!-- CK_CUR_SUBSECTION={6} -->

<h3>A.6&nbsp;&nbsp;Experimental methodology</h3>

<!-- CK_LABEL={experimental_methodology} -->


<p>One of the most important points of using Collective Knowledge framework
is to take advantage of the experimental methodology for computer systems research
continuously improved by the community.
For this purpose, we instrument programs using a small xOpenME library&nbsp;[<a href="#ref_4">4</a>]
which allows us to monitor behavior of some code regions and dump 
final statistics to a JSON file in the CK format.
CK will then repeat each autotuning iteration N times,
apply statistical analysis on all exposed characteristics, report min, max and mean values,
and calculate expected value based on a histogram of all results (if supported by used Python)
as shown in Figure&nbsp;<a href="#fig_reproducibility">37</a> (a).
<p>We then calculate improvements of a given optimization over reference one (-O3)
using minimal and expected execution times, and record differences.
If the difference is more than 5&#37;, we mark such experient is noise and untrustable 
to be analyzed and improved later by the community.
If several system states are detected as shown in Figure&nbsp;<a href="#fig_reproducibility">37</a> (b),
CK will not be able to reproduce them - it then means that the common CK experimental workflow
should also be improved for this hardware and environment to be able to distinguish such
states (such as CPU and GPU frequency due to DVFS for example).
<p>


<a name="notes">

<!-- CK_CUR_SUBSECTION={7} -->

<h3>A.7&nbsp;&nbsp;Notes</h3>

<!-- CK_LABEL={notes} -->


<p>We did not have time to thoroughly document experiments from sections&nbsp;<a href="#sec_crowdfuzzing">7</a>+ of this report.
However we shared all CK modules, workflows and scripts we used in this report 
in the following CK entries:
<p>

<pre>$ ck ls script:rpi3-*</pre> 

<pre>$ ck ls converting-ad-hoc-works-to-ck-*</pre> 



<p>Scripts from Sections&nbsp;<a href="#sec_autotuning">3</a> and <a href="#sec_flag_autotuning">4</a>
to invoke portable and customizable CK autotuning workflow:
<p>

<pre>$ ck find script:rpi3-susan-autotune</pre> 

<pre>$ ck find script:rpi3-susan-graphs</pre> 

<pre>$ ck find script:rpi3-susan-reduce</pre> 

<pre>$ ck find script:rpi3-all-autotune</pre> 



<p>Scripts from Section&nbsp;<a href="#sec_crowdtuning">5</a>:
<p>

<pre>$ ck find script:rpi3-all-autotune</pre> 

<pre>$ ck find script:rpi3-crowdtune</pre> 



<p>Scripts from Section&nbsp;<a href="#sec_collaborative">6</a>:
<p>

<pre>$ ck find script:rpi3-zlib-decode-autotune</pre> 

<pre>$ ck find script:rpi3-zlib-decode-graphs</pre> 

<pre>$ ck find script:rpi3-zlib-decode-reduce</pre> 

<pre>$ ck find script:rpi3-zlib-encode-autotune</pre> 

<pre>$ ck find script:rpi3-zlib-encode-graphs</pre> 

<pre>$ ck find script:rpi3-zlib-encode-reduce</pre> 



<p>Scripts from Section&nbsp;<a href="#sec_crowdfuzzing">7</a>:
<p>

<pre>$ ck find script:rpi3-susan-fuzz-bugs</pre> 



<p>Scripts from Sections&nbsp;<a href="#sec_crowdmodeling">8</a> and <a href="#sec_features">9</a>:
<p>

<pre>$ ck find script:rpi3-crowdmodel</pre> 



<p>Scripts from Section&nbsp;<a href="#sec_datasets">10</a>: //input-aware
<p>

<pre>$ ck find script:rpi3-all-autotune-multiple-datasets</pre> 

<pre>$ ck find script:rpi3-input-aware-autotune-blas</pre> 



<p>Scripts from Section&nbsp;<a href="#sec_competitions">11</a>:
<p>

<pre>$ ck find script:converting-ad-hoc-works-to-ck-slambench-autotuning</pre> 



<p>



<!-- CK_CUR_SUBSECTION={8} -->

<h3>A.8&nbsp;&nbsp;Conclusion</h3>

<p>We hope that our customizable autotuning and machine learning workflow 
can teach students, scientists and engineers learn how to collaboratively
co-design Pareto-efficient software and hardware stack for emerging workloads.
Please feel free to send us updates and patches to fix, help us improve or extend
our artifacts with documentation, and keep in touch with our community via
CK mailing list:&nbsp;<a href="https://groups.google.com/d/forum/collective-knowledge">groups.google.com/d/forum/collective-knowledge</a>!
<p>
<p>
<p>
<br><br>
<h2>References</h2>
<table border="0" cellpadding="5" cellspacing="0">
 <tr>
  <td valign="top"><a name="ref_1"><b>[1]</b></td>  <td valign="top">The HiPEAC vision on high-performance and embedded architecture and
  compilation (2012-2020).
<br> http://www.hipeac.net/roadmap, 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_2"><b>[2]</b></td>  <td valign="top">J.&nbsp;Dongarra&nbsp;et.al.
<br> The international exascale software project roadmap.
<br>  Int. J. High Perform. Comput. Appl., 25(1):3--60, Feb. 2011.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_3"><b>[3]</b></td>  <td valign="top">PRACE: partnership for advanced computing in europe.
<br> http://www.prace-project.eu.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_4"><b>[4]</b></td>  <td valign="top">G.&nbsp;Fursin, R.&nbsp;Miceli, A.&nbsp;Lokhmotov, M.&nbsp;Gerndt, M.&nbsp;Baboulin, D.&nbsp;Malony, Allen,
  Z.&nbsp;Chamski, D.&nbsp;Novillo, and D.&nbsp;D. Vento.
<br> Collective Mind: Towards practical and collaborative auto-tuning.
<br>  Scientific Programming, 22(4):309--329, July 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_5"><b>[5]</b></td>  <td valign="top">G.&nbsp;Fursin.
<br> Collective Tuning Initiative: automating and accelerating
  development and optimization of computing systems.
<br> In  Proceedings of the GCC Developers' Summit, June 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_6"><b>[6]</b></td>  <td valign="top">G.&nbsp;Fursin, A.&nbsp;Memon, C.&nbsp;Guillon, and A.&nbsp;Lokhmotov.
<br> Collective Mind, Part II: Towards performance- and cost-aware
  software engineering as a natural science.
<br> In  18th International Workshop on Compilers for Parallel
  Computing (CPC'15), January 2015.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_7"><b>[7]</b></td>  <td valign="top">Artifact Evaluation for Computer Systems Conferences including CGO,PPoPP,PACT
  and SuperComputing: developing common experimental methodology and tools for
  reproducible and sustainable research.
<br> <a href="http://cTuning.org/ae">Link</a>, 2014-cur.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_8"><b>[8]</b></td>  <td valign="top">B.&nbsp;R. Childers, G.&nbsp;Fursin, S.&nbsp;Krishnamurthi, and A.&nbsp;Zeller.
<br> Artifact Evaluation for Publications (Dagstuhl Perspectives Workshop
  15452).
<br> 5(11), 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_9"><b>[9]</b></td>  <td valign="top">G.&nbsp;Fursin and C.&nbsp;Dubach.
<br> Community-driven reviewing and validation of publications.
<br> In  Proceedings of the 1st Workshop on Reproducible Research
  Methodologies and New Publication Models in Computer Engineering (ACM SIGPLAN
  TRUST'14). ACM, 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_10"><b>[10]</b></td>  <td valign="top">R.&nbsp;Whaley and J.&nbsp;Dongarra.
<br> Automatically tuned linear algebra software.
<br> In  Proceedings of the Conference on High Performance Networking
  and Computing, 1998.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_11"><b>[11]</b></td>  <td valign="top">F.&nbsp;Matteo and S.&nbsp;Johnson.
<br> FFTW: An adaptive software architecture for the FFT.
<br> In  Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing, volume&nbsp;3, pages 1381--1384,
  Seattle, WA, May 1998.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_12"><b>[12]</b></td>  <td valign="top">K.&nbsp;Cooper, P.&nbsp;Schielke, and D.&nbsp;Subramanian.
<br> Optimizing for reduced code space using genetic algorithms.
<br> In  Proceedings of the Conference on Languages, Compilers, and
  Tools for Embedded Systems (LCTES), pages 1--9, 1999.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_13"><b>[13]</b></td>  <td valign="top">M.&nbsp;Voss and R.&nbsp;Eigenmann.
<br> ADAPT: Automated de-coupled adaptive program transformation.
<br> In  Proceedings of International Conference on Parallel
  Processing, 2000.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_14"><b>[14]</b></td>  <td valign="top">G.&nbsp;Fursin, M.&nbsp;O'Boyle, and P.&nbsp;Knijnenburg.
<br> Evaluating iterative compilation.
<br> In  Proceedings of the Workshop on Languages and Compilers for
  Parallel Computers (LCPC), pages 305--315, 2002.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_15"><b>[15]</b></td>  <td valign="top">C.&nbsp;&Tcedil;&abreve;pu&scedil;, I.-H. Chung, and J.&nbsp;K. Hollingsworth.
<br> Active harmony: towards automated performance tuning.
<br> In  Proceedings of the 2002 ACM/IEEE conference on
  Supercomputing, Supercomputing '02, pages 1--11, Los Alamitos, CA, USA,
  2002. IEEE Computer Society Press.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_16"><b>[16]</b></td>  <td valign="top">P.&nbsp;Kulkarni, W.&nbsp;Zhao, H.&nbsp;Moon, K.&nbsp;Cho, D.&nbsp;Whalley, J.&nbsp;Davidson, M.&nbsp;Bailey,
  Y.&nbsp;Paek, and K.&nbsp;Gallivan.
<br> Finding effective optimization phase sequences.
<br> In  Proceedings of the Conference on Languages, Compilers, and
  Tools for Embedded Systems (LCTES), pages 12--23, 2003.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_17"><b>[17]</b></td>  <td valign="top">B.&nbsp;Singer and M.&nbsp;Veloso.
<br> Learning to predict performance from formula modeling and training
  data.
<br> In  Proceedings of the Conference on Machine Learning, 2000.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_18"><b>[18]</b></td>  <td valign="top">J.&nbsp;Lu, H.&nbsp;Chen, P.-C. Yew, and W.-C. Hsu.
<br> Design and implementation of a lightweight dynamic optimization
  system.
<br> In  Journal of Instruction-Level Parallelism, volume&nbsp;6, 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_19"><b>[19]</b></td>  <td valign="top">C.&nbsp;Lattner and V.&nbsp;Adve.
<br> LLVM: A compilation framework for lifelong program analysis &
  transformation.
<br> In  Proceedings of the 2004 International Symposium on Code
  Generation and Optimization (CGO'04), Palo Alto, California, March 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_20"><b>[20]</b></td>  <td valign="top">Z.&nbsp;Pan and R.&nbsp;Eigenmann.
<br> Fast and effective orchestration of compiler optimizations for
  automatic performance tuning.
<br> In  Proceedings of the International Symposium on Code Generation
  and Optimization (CGO), pages 319--332, 2006.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_21"><b>[21]</b></td>  <td valign="top">S.&nbsp;S. Shende and A.&nbsp;D. Malony.
<br> The tau parallel performance system.
<br>  Int. J. High Perform. Comput. Appl., 20(2):287--311, May 2006.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_22"><b>[22]</b></td>  <td valign="top">D.&nbsp;Bailey, J.&nbsp;Chame, C.&nbsp;Chen, J.&nbsp;Dongarra, M.&nbsp;Hall, J.&nbsp;Hollingsworth,
  P.&nbsp;Hovland, S.&nbsp;Moore, K.&nbsp;Seymour, J.&nbsp;Shin, A.&nbsp;Tiwari, S.&nbsp;Williams, and
  H.&nbsp;You.
<br> PERI auto-tuning.
<br>  Journal of Physics: Conference Series, 125(1):012089, 2008.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_23"><b>[23]</b></td>  <td valign="top">A.&nbsp;Hartono, B.&nbsp;Norris, and P.&nbsp;Sadayappan.
<br> Annotation-based empirical performance tuning using orio.
<br> In  23rd IEEE International Symposium on Parallel and
  Distributed Processing, IPDPS 2009, Rome, Italy, May 23-29, 2009, pages
  1--11, 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_24"><b>[24]</b></td>  <td valign="top">G.&nbsp;Fursin, Y.&nbsp;Kashnikov, A.&nbsp;W. Memon, Z.&nbsp;Chamski, O.&nbsp;Temam, M.&nbsp;Namolaru,
  E.&nbsp;Yom-Tov, B.&nbsp;Mendelson, A.&nbsp;Zaks, E.&nbsp;Courtois, F.&nbsp;Bodin, P.&nbsp;Barnard,
  E.&nbsp;Ashton, E.&nbsp;Bonilla, J.&nbsp;Thomson, C.&nbsp;Williams, and M.&nbsp;F.&nbsp;P. O'Boyle.
<br> Milepost gcc: Machine learning enabled self-tuning compiler.
<br>  International Journal of Parallel Programming (IJPP),
  39:296--327, 2011.
<br> 10.1007/s10766-010-0161-2.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_25"><b>[25]</b></td>  <td valign="top">S.&nbsp;Tomov, R.&nbsp;Nath, H.&nbsp;Ltaief, and J.&nbsp;Dongarra.
<br> Dense linear algebra solvers for multicore with GPU accelerators.
<br> In  Proc. of the IEEE IPDPS'10, pages 1--8, Atlanta, GA, April
  19-23 2010. IEEE Computer Society.
<br> DOI:&nbsp;10.1109/IPDPSW.2010.5470941.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_26"><b>[26]</b></td>  <td valign="top">Open benchmarking: automated testing & benchmarking on an open platform.
<br> <a href="http://openbenchmarking.org">Link</a>, 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_27"><b>[27]</b></td>  <td valign="top">G.&nbsp;Ren, E.&nbsp;Tune, T.&nbsp;Moseley, Y.&nbsp;Shi, S.&nbsp;Rus, and R.&nbsp;Hundt.
<br> Google-wide profiling: A continuous profiling infrastructure for data
  centers.
<br>  IEEE Micro, 30(4):65--79, July 2010.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_28"><b>[28]</b></td>  <td valign="top">S.&nbsp;Grauer-Gray, L.&nbsp;Xu, R.&nbsp;Searles, S.&nbsp;Ayalasomayajula, and J.&nbsp;Cavazos.
<br> Auto-tuning a high-level language targeted to GPU codes.
<br> In  Innovative Parallel Computing (InPar), 2012, pages 1--10,
  May 2012.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_29"><b>[29]</b></td>  <td valign="top">D.&nbsp;Grewe, Z.&nbsp;Wang, and M.&nbsp;F.&nbsp;P. O'Boyle.
<br> Portable mapping of data parallel programs to opencl for
  heterogeneous systems.
<br> In  Proceedings of the 2013 IEEE/ACM International Symposium on
  Code Generation and Optimization, CGO 2013, Shenzhen, China, February
  23-27, 2013, pages 22:1--22:10, 2013.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_30"><b>[30]</b></td>  <td valign="top">M.&nbsp;Khan, P.&nbsp;Basu, G.&nbsp;Rudy, M.&nbsp;Hall, C.&nbsp;Chen, and J.&nbsp;Chame.
<br> A script-based autotuning compiler system to generate
  high-performance cuda code.
<br>  ACM Trans. Archit. Code Optim., 9(4):31:1--31:25, Jan. 2013.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_31"><b>[31]</b></td>  <td valign="top">J.&nbsp;Ansel, S.&nbsp;Kamil, K.&nbsp;Veeramachaneni, J.&nbsp;Ragan-Kelley, J.&nbsp;Bosboom, U.-M.
  O'Reilly, and S.&nbsp;Amarasinghe.
<br> Opentuner: An extensible framework for program autotuning.
<br> In  International Conference on Parallel Architectures and
  Compilation Techniques, Edmonton, Canada, August 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_32"><b>[32]</b></td>  <td valign="top">Y.&nbsp;M. Tsai, P.&nbsp;Luszczek, J.&nbsp;Kurzak, and J.&nbsp;J. Dongarra.
<br> Performance-portable autotuning of opencl kernels for convolutional
  layers of deep neural networks.
<br> In  2nd Workshop on Machine Learning in HPC Environments,
  MLHPC@SC, Salt Lake City, UT, USA, November 14, 2016, pages 9--18, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_33"><b>[33]</b></td>  <td valign="top">A.&nbsp;Abdelfattah, A.&nbsp;Haidar, S.&nbsp;Tomov, and J.&nbsp;J. Dongarra.
<br> Performance, design, and autotuning of batched GEMM for gpus.
<br> In  High Performance Computing - 31st International Conference,
  ISC High Performance 2016, Frankfurt, Germany, June 19-23, 2016,
  Proceedings, pages 21--38, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_34"><b>[34]</b></td>  <td valign="top">Collective Knowledge: open-source, customizable and cross-platform workflow
  framework and repository for computer systems research.
<br> <a href="https://github.com/ctuning/ck">Link</a>, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_35"><b>[35]</b></td>  <td valign="top">G.&nbsp;Fursin, A.&nbsp;Lokhmotov, and E.&nbsp;Plowman.
<br> Collective Knowledge: towards R&D sustainability.
<br> In  Proceedings of the Conference on Design, Automation and Test
  in Europe (DATE'16), March 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_36"><b>[36]</b></td>  <td valign="top">Introducing JSON.
<br> http://www.json.org.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_37"><b>[37]</b></td>  <td valign="top">D.&nbsp;P. Anderson, J.&nbsp;Cobb, E.&nbsp;Korpela, M.&nbsp;Lebofsky, and D.&nbsp;Werthimer.
<br> Seti@home: An experiment in public-resource computing.
<br>  Commun. ACM, 45(11):56--61, Nov. 2002.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_38"><b>[38]</b></td>  <td valign="top">Open collective knowledge repository with shared optimization results from
  crowdsourced experiments across diverse platforms and data sets.
<br> <a href="http://cKnowledge.org/repo">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_39"><b>[39]</b></td>  <td valign="top">Public optimization results when crowd-tuning gcc 7.1.0 across raspberry pi3
  devices.
<br>
  <a href="http://cKnowledge.org/repo/web.php?wcid=8289e0cf24346aa7:79bca2b76876b5c6">link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_40"><b>[40]</b></td>  <td valign="top">ReQuEST: open tournaments on collaborative, reproducible and pareto-efficient
  software/hardware co-design of emerging workloads such as deep learning using
  collective knowledge technology.
<br> <a href="http://cKnowledge.org/request">Link</a>, 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_41"><b>[41]</b></td>  <td valign="top">MILEPOST project archive (MachIne Learning for Embedded PrOgramS
  opTimization).
<br> <a href="http://cTuning.org/project-milepost">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_42"><b>[42]</b></td>  <td valign="top">cTuning.org: public portal for collaborative and reproducible computer
  engineering.
<br> <a href="http://cTuning.org">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_43"><b>[43]</b></td>  <td valign="top">Zenodo - research data repository.
<br> <a href="http://zenodo.org">Link</a>, 2013.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_44"><b>[44]</b></td>  <td valign="top">Figshare - online digital repository where researchers can preserve and share
  their research outputs, including figures, datasets, images, and videos.
<br> <a href="http://figshare.com">Link</a>, 2011.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_45"><b>[45]</b></td>  <td valign="top">Digital Object Identifier or DOI - a persistent identifier or handle used to
  uniquely identify objects, standardized by the iso.
<br> <a href="http://doi.org">Link</a>, 2000.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_46"><b>[46]</b></td>  <td valign="top">Proceedings of the 1st workshop on reproducible research methodologies and new
  publication models in computer engineering (acm sigplan trust'14).
<br> ACM, 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_47"><b>[47]</b></td>  <td valign="top">T.&nbsp;Oinn, M.&nbsp;Addis, J.&nbsp;Ferris, D.&nbsp;Marvin, M.&nbsp;Senger, M.&nbsp;Greenwood, T.&nbsp;Carver,
  K.&nbsp;Glover, M.&nbsp;R. Pocock, A.&nbsp;Wipat, and P.&nbsp;Li.
<br> Taverna: a tool for the composition and enactment of bioinformatics
  workflows.
<br>  Bioinformatics, 20(17):3045--3054, 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_48"><b>[48]</b></td>  <td valign="top">J.&nbsp;E. Smith and R.&nbsp;Nair.
<br> The architecture of virtual machines.
<br>  Computer, 38(5):32--38, May 2005.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_49"><b>[49]</b></td>  <td valign="top">Docker: open source lightweight container technology that can run processes
  in isolation.
<br> <a href="http://www.docker.org">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_50"><b>[50]</b></td>  <td valign="top">TETRACOM - eu fp7 project to support technology transfer in computing
  systems.
<br> <a href="https://www.tetracom.eu">Link</a>, 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_51"><b>[51]</b></td>  <td valign="top">Blog article: CK concepts by Michel Steuwer.
<br> <a href="http://michel.steuwer.info/About-CK">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_52"><b>[52]</b></td>  <td valign="top">Ck repository with multi-platform software and package manager implemented as
  ck modules which detect or install various software (compilers, libraries,
  tools).
<br> <a href="https://github.com/ctuning/ck-env">Link</a>, 2015.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_53"><b>[53]</b></td>  <td valign="top">T.&nbsp;Gamblin, M.&nbsp;LeGendre, M.&nbsp;R. Collette, G.&nbsp;L. Lee, A.&nbsp;Moody, B.&nbsp;R.
  de&nbsp;Supinski, and S.&nbsp;Futral.
<br> The spack package manager: Bringing order to hpc software chaos.
<br> In  Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis, SC '15, pages
  40:1--40:12, New York, NY, USA, 2015. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_54"><b>[54]</b></td>  <td valign="top">K.&nbsp;Hoste, J.&nbsp;Timmerman, A.&nbsp;Georges, and S.&nbsp;D. Weirdt.
<br> Easybuild: Building software with ease.
<br> In  2012 SC Companion: High Performance Computing, Networking
  Storage and Analysis, Salt Lake City, UT, USA, November 10-16, 2012, pages
  572--582, 2012.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_55"><b>[55]</b></td>  <td valign="top">S.&nbsp;Ainsworth and T.&nbsp;M. Jones.
<br> Software prefetching for indirect memory accesses.
<br> In  Proceedings of the 2017 International Symposium on Code
  Generation and Optimization, CGO '17, pages 305--317, Piscataway, NJ, USA,
  2017. IEEE Press.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_56"><b>[56]</b></td>  <td valign="top">Artifacts and experimental workflows in the Collective Knowledge Format for
  the CGO'17 paper "Software Prefetching for Indirect Memory Accesses".
<br> <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">Link</a>, 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_57"><b>[57]</b></td>  <td valign="top">D.&nbsp;Wilkinson, B.&nbsp;Childers, R.&nbsp;Bernard, W.&nbsp;Graves, and J.&nbsp;Davidson.
<br> Acm pilot demo 1 - collective knowledge: Packaging and sharing.
  version 3.
<br> 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_58"><b>[58]</b></td>  <td valign="top">B.&nbsp;Aarts and et.al.
<br> OCEANS: Optimizing compilers for embedded applications.
<br> In  Proc. Euro-Par 97, volume 1300 of  Lecture Notes in
  Computer Science, pages 1351--1356, 1997.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_59"><b>[59]</b></td>  <td valign="top">B.&nbsp;Calder, D.&nbsp;Grunwald, M.&nbsp;Jones, D.&nbsp;Lindsay, J.&nbsp;Martin, M.&nbsp;Mozer, and B.&nbsp;Zorn.
<br> Evidence-based static branch prediction using machine learning.
<br>  ACM Transactions on Programming Languages and Systems (TOPLAS),
  1997.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_60"><b>[60]</b></td>  <td valign="top">A.&nbsp;Nisbet.
<br> Iterative feedback directed parallelisation using genetic algorithms.
<br> In  Proceedings of the Workshop on Profile and Feedback Directed
  Compilation in conjunction with International Conference on Parallel
  Architectures and Compilation Technique (PACT), 1998.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_61"><b>[61]</b></td>  <td valign="top">T.&nbsp;Kisuki, P.&nbsp;Knijnenburg, and M.&nbsp;O'Boyle.
<br> Combined selection of tile sizes and unroll factors using iterative
  compilation.
<br> In  Proceedings of the International Conference on Parallel
  Architectures and Compilation Techniques (PACT), pages 237--246, 2000.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_62"><b>[62]</b></td>  <td valign="top">M.&nbsp;Stephenson, S.&nbsp;Amarasinghe, M.&nbsp;Martin, and U.-M. O'Reilly.
<br> Meta optimization: Improving compiler heuristics with machine
  learning.
<br> In  Proceedings of the ACM SIGPLAN Conference on Programming
  Language Design and Implementation (PLDI'03), pages 77--90, June 2003.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_63"><b>[63]</b></td>  <td valign="top">B.&nbsp;Franke, M.&nbsp;O'Boyle, J.&nbsp;Thomson, and G.&nbsp;Fursin.
<br> Probabilistic source-level optimisation of embedded programs.
<br> In  Proceedings of the Conference on Languages, Compilers, and
  Tools for Embedded Systems (LCTES), 2005.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_64"><b>[64]</b></td>  <td valign="top">K.&nbsp;Hoste and L.&nbsp;Eeckhout.
<br> Cole: Compiler optimization level exploration.
<br> In  Proceedings of the International Symposium on Code Generation
  and Optimization (CGO), 2008.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_65"><b>[65]</b></td>  <td valign="top">Bailey and et.al.
<br> Peri auto-tuning.
<br>  Journal of Physics: Conference Series (SciDAC 2008), 125:1--6,
  2008.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_66"><b>[66]</b></td>  <td valign="top">V.&nbsp;Jimenez, I.&nbsp;Gelado, L.&nbsp;Vilanova, M.&nbsp;Gil, G.&nbsp;Fursin, and N.&nbsp;Navarro.
<br> Predictive runtime code scheduling for heterogeneous architectures.
<br> In  Proceedings of the International Conference on High
  Performance Embedded Architectures & Compilers (HiPEAC 2009), January 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_67"><b>[67]</b></td>  <td valign="top">J.&nbsp;Ansel, C.&nbsp;Chan, Y.&nbsp;L. Wong, M.&nbsp;Olszewski, Q.&nbsp;Zhao, A.&nbsp;Edelman, and
  S.&nbsp;Amarasinghe.
<br> Petabricks: a language and compiler for algorithmic choice.
<br> In  Proceedings of the 2009 ACM SIGPLAN conference on Programming
  language design and implementation, PLDI '09, pages 38--49, New York, NY,
  USA, 2009. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_68"><b>[68]</b></td>  <td valign="top">J.&nbsp;Mars, N.&nbsp;Vachharajani, R.&nbsp;Hundt, and M.&nbsp;L. Soffa.
<br> Contention aware execution: Online contention detection and response.
<br> In  Proceedings of the 8th Annual IEEE/ACM International
  Symposium on Code Generation and Optimization, CGO '10, pages 257--265, New
  York, NY, USA, 2010. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_69"><b>[69]</b></td>  <td valign="top">R.&nbsp;W. Moore and B.&nbsp;R. Childers.
<br> Automatic generation of program affinity policies using machine
  learning.
<br> In  CC, pages 184--203, 2013.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_70"><b>[70]</b></td>  <td valign="top">J.&nbsp;Shen, A.&nbsp;L. Varbanescu, H.&nbsp;J. Sips, M.&nbsp;Arntzen, and D.&nbsp;G. Simons.
<br> Glinda: a framework for accelerating imbalanced applications on
  heterogeneous platforms.
<br> In  Conf. Computing Frontiers, page&nbsp;14, 2013.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_71"><b>[71]</b></td>  <td valign="top">R.&nbsp;Miceli&nbsp;et.al.
<br> Autotune: A plugin-driven approach to the automatic tuning of
  parallel applications.
<br> In  Proceedings of the 11th International Conference on Applied
  Parallel and Scientific Computing, PARA'12, pages 328--342, Berlin,
  Heidelberg, 2013. Springer-Verlag.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_72"><b>[72]</b></td>  <td valign="top">I.&nbsp;Manotas, L.&nbsp;Pollock, and J.&nbsp;Clause.
<br> Seeds: A software engineer's energy-optimization decision support
  framework.
<br> In  Proceedings of the 36th International Conference on Software
  Engineering, ICSE 2014, pages 503--514, New York, NY, USA, 2014. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_73"><b>[73]</b></td>  <td valign="top">A.&nbsp;H. Ashouri, G.&nbsp;Mariani, G.&nbsp;Palermo, E.&nbsp;Park, J.&nbsp;Cavazos, and C.&nbsp;Silvano.
<br> Cobayn: Compiler autotuning framework using bayesian networks.
<br>  ACM Transactions on Architecture and Code Optimization (TACO),
  13(2):21, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_74"><b>[74]</b></td>  <td valign="top">F.&nbsp;Pedregosa, G.&nbsp;Varoquaux, A.&nbsp;Gramfort, V.&nbsp;Michel, B.&nbsp;Thirion, O.&nbsp;Grisel,
  M.&nbsp;Blondel, P.&nbsp;Prettenhofer, R.&nbsp;Weiss, V.&nbsp;Dubourg, J.&nbsp;Vanderplas, A.&nbsp;Passos,
  D.&nbsp;Cournapeau, M.&nbsp;Brucher, M.&nbsp;Perrot, and E.&nbsp;Duchesnay.
<br> Scikit-learn: Machine learning in Python.
<br>  Journal of Machine Learning Research, 12:2825--2830, 2011.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_75"><b>[75]</b></td>  <td valign="top">G.&nbsp;Fursin, M.&nbsp;O'Boyle, O.&nbsp;Temam, and G.&nbsp;Watts.
<br> Fast and accurate method for determining a lower bound on execution
  time.
<br>  Concurrency: Practice and Experience, 16(2-3):271--292, 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_76"><b>[76]</b></td>  <td valign="top">K.&nbsp;Asanovic, R.&nbsp;Bodik, B.&nbsp;C. Catanzaro, J.&nbsp;J. Gebis, P.&nbsp;Husbands, K.&nbsp;Keutzer,
  D.&nbsp;A. Patterson, W.&nbsp;L. Plishker, J.&nbsp;Shalf, S.&nbsp;W. Williams, and K.&nbsp;A. Yelick.
<br> The landscape of parallel computing research: a view from Berkeley.
<br> Technical Report UCB/EECS-2006-183, Electrical Engineering and
  Computer Sciences, University of California at Berkeley, Dec. 2006.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_77"><b>[77]</b></td>  <td valign="top">M.&nbsp;Hall, D.&nbsp;Padua, and K.&nbsp;Pingali.
<br> Compiler research: The next 50 years.
<br>  Commun. ACM, 52(2):60--67, Feb. 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_78"><b>[78]</b></td>  <td valign="top">J.&nbsp;W. Duran and S.&nbsp;Ntafos.
<br> A report on random testing.
<br> In  Proceedings of the 5th International Conference on Software
  Engineering, ICSE '81, pages 179--183, Piscataway, NJ, USA, 1981. IEEE
  Press.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_79"><b>[79]</b></td>  <td valign="top">A.&nbsp;Takanen, J.&nbsp;DeMott, and C.&nbsp;Miller.
<br>  Fuzzing for Software Security Testing and Quality Assurance.
<br> Artech House, Inc., Norwood, MA, USA, 1 edition, 2008.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_80"><b>[80]</b></td>  <td valign="top">X.&nbsp;Yang, Y.&nbsp;Chen, E.&nbsp;Eide, and J.&nbsp;Regehr.
<br> Finding and understanding bugs in c compilers.
<br> In  Proceedings of the 32Nd ACM SIGPLAN Conference on Programming
  Language Design and Implementation, PLDI '11, pages 283--294, New York, NY,
  USA, 2011. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_81"><b>[81]</b></td>  <td valign="top">Microsoft security risk detection.
<br> <a href="https://www.microsoft.com/en-us/security-risk-detection/">Link</a>, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_82"><b>[82]</b></td>  <td valign="top">Continuous fuzzing of open source software.
<br> <a href="https://github.com/google/oss-fuzz">Link</a>, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_83"><b>[83]</b></td>  <td valign="top">C.&nbsp;Lidbury, A.&nbsp;Lascu, N.&nbsp;Chong, and A.&nbsp;F. Donaldson.
<br> Many-core compiler fuzzing.
<br> In  Proceedings of the 36th ACM SIGPLAN Conference on Programming
  Language Design and Implementation, PLDI '15, pages 65--76, New York, NY,
  USA, 2015. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_84"><b>[84]</b></td>  <td valign="top">A.&nbsp;Lascu and A.&nbsp;F. Donaldson.
<br> Integrating a large-scale testing campaign in the CK framework.
<br>  CoRR, abs/1511.02725, 2015.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_85"><b>[85]</b></td>  <td valign="top">Ck workflow for opencl crowd-fuzzing.
<br> <a href="https://github.com/ctuning/ck-clsmith">Link</a>, 2015.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_86"><b>[86]</b></td>  <td valign="top">C.&nbsp;M. Bishop.
<br>  Pattern Recognition and Machine Learning (Information Science
  and Statistics).
<br> Springer, 1st ed. 2006. corr. 2nd printing 2011 edition, Oct. 2007.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_87"><b>[87]</b></td>  <td valign="top">C.&nbsp;Sammut and G.&nbsp;Webb.
<br>  Encyclopedia of Machine Learning and Data Mining.
<br> Springer reference. Springer Science + Business Media.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_88"><b>[88]</b></td>  <td valign="top">J.&nbsp;Cavazos, G.&nbsp;Fursin, F.&nbsp;Agakov, E.&nbsp;Bonilla, M.&nbsp;O'Boyle, and O.&nbsp;Temam.
<br> Rapidly selecting good compiler optimizations using performance
  counters.
<br> In  Proceedings of the International Symposium on Code Generation
  and Optimization (CGO), March 2007.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_89"><b>[89]</b></td>  <td valign="top">O.&nbsp;Russakovsky, J.&nbsp;Deng, H.&nbsp;Su, J.&nbsp;Krause, S.&nbsp;Satheesh, S.&nbsp;Ma, Z.&nbsp;Huang,
  A.&nbsp;Karpathy, A.&nbsp;Khosla, M.&nbsp;S. Bernstein, A.&nbsp;C. Berg, and F.&nbsp;Li.
<br> Imagenet large scale visual recognition challenge.
<br>  CoRR, abs/1409.0575, 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_90"><b>[90]</b></td>  <td valign="top">A.&nbsp;Monsifrot, F.&nbsp;Bodin, and R.&nbsp;Quiniou.
<br> A machine learning approach to automatic production of compiler
  heuristics.
<br> In  Proceedings of the International Conference on Artificial
  Intelligence: Methodology, Systems, Applications, LNCS 2443, pages 41--50,
  2002.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_91"><b>[91]</b></td>  <td valign="top">G.&nbsp;Marin and J.&nbsp;Mellor-Crummey.
<br> Cross-architecture performance predictions for scientific
  applications using parameterized models.
<br>  SIGMETRICS Perform. Eval. Rev., 32(1):2--13, June 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_92"><b>[92]</b></td>  <td valign="top">M.&nbsp;Stephenson and S.&nbsp;Amarasinghe.
<br> Predicting unroll factors using supervised classification.
<br> In  Proceedings of the International Symposium on Code Generation
  and Optimization (CGO). IEEE Computer Society, 2005.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_93"><b>[93]</b></td>  <td valign="top">M.&nbsp;Zhao, B.&nbsp;R. Childers, and M.&nbsp;L. Soffa.
<br> A model-based framework: an approach for profit-driven optimization.
<br> In  Third Annual IEEE/ACM Interational Conference on Code
  Generation and Optimization, pages 317--327, 2005.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_94"><b>[94]</b></td>  <td valign="top">F.&nbsp;Agakov, E.&nbsp;Bonilla, J.Cavazos, B.Franke, G.&nbsp;Fursin, M.&nbsp;O'Boyle, J.&nbsp;Thomson,
  M.&nbsp;Toussaint, and C.&nbsp;Williams.
<br> Using machine learning to focus iterative optimization.
<br> In  Proceedings of the International Symposium on Code Generation
  and Optimization (CGO), 2006.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_95"><b>[95]</b></td>  <td valign="top">C.&nbsp;Dubach, T.&nbsp;M. Jones, E.&nbsp;V. Bonilla, G.&nbsp;Fursin, and M.&nbsp;F. O'Boyle.
<br> Portable compiler optimization across embedded programs and
  microarchitectures using machine learning.
<br> In  Proceedings of the IEEE/ACM International Symposium on
  Microarchitecture (MICRO), December 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_96"><b>[96]</b></td>  <td valign="top">E.&nbsp;Park, J.&nbsp;Cavazos, L.-N. Pouchet, C.&nbsp;Bastoul, A.&nbsp;Cohen, and P.&nbsp;Sadayappan.
<br> Predictive modeling in a polyhedral optimization space.
<br>  International Journal of Parallel Programming, 41(5):704--750,
  2013.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_97"><b>[97]</b></td>  <td valign="top">H.&nbsp;Leather, E.&nbsp;V. Bonilla, and M.&nbsp;F.&nbsp;P. O'Boyle.
<br> Automatic feature generation for machine learning-based optimising
  compilation.
<br>  TACO, 11(1):14:1--14:32, 2014.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_98"><b>[98]</b></td>  <td valign="top">C.&nbsp;Cummins, P.&nbsp;Petoumenos, Z.&nbsp;Wang, and H.&nbsp;Leather.
<br> End-to-end deep learning of optimization heuristics.
<br> In  26th International Conference on Parallel Architectures and
  Compilation Techniques, PACT 2017, Portland, OR, USA, September 9-13,
  2017, pages 219--232, 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_99"><b>[99]</b></td>  <td valign="top">A.&nbsp;H. Ashouri, A.&nbsp;Bignoli, G.&nbsp;Palermo, C.&nbsp;Silvano, S.&nbsp;Kulkarni, and J.&nbsp;Cavazos.
<br> Micomp: Mitigating the compiler phase-ordering problem using
  optimization sub-sequences and machine learning.
<br>  ACM Trans. Archit. Code Optim., 14(3):29:1--29:28, Sept. 2017.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_100"><b>[100]</b></td>  <td valign="top">Artifact evaluation for computer systems research.
<br> <a href="http://ctuning.org/ae">Link</a>, 2014-cur.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_101"><b>[101]</b></td>  <td valign="top">Static Features available in MILEPOST GCC V2.1.
<br>
  <a href="http://ctuning.org/wiki/index.php/CTools:MilepostGCC:StaticFeatures:MILEPOST_V2.1">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_102"><b>[102]</b></td>  <td valign="top">M.&nbsp;Abadi, A.&nbsp;Agarwal, P.&nbsp;Barham, E.&nbsp;Brevdo, Z.&nbsp;Chen, C.&nbsp;Citro, G.&nbsp;S. Corrado,
  A.&nbsp;Davis, J.&nbsp;Dean, M.&nbsp;Devin, S.&nbsp;Ghemawat, I.&nbsp;J. Goodfellow, A.&nbsp;Harp,
  G.&nbsp;Irving, M.&nbsp;Isard, Y.&nbsp;Jia, R.&nbsp;J\'ozefowicz, L.&nbsp;Kaiser, M.&nbsp;Kudlur,
  J.&nbsp;Levenberg, D.&nbsp;Man\'e, R.&nbsp;Monga, S.&nbsp;Moore, D.&nbsp;G. Murray, C.&nbsp;Olah,
  M.&nbsp;Schuster, J.&nbsp;Shlens, B.&nbsp;Steiner, I.&nbsp;Sutskever, K.&nbsp;Talwar, P.&nbsp;A. Tucker,
  V.&nbsp;Vanhoucke, V.&nbsp;Vasudevan, F.&nbsp;B. Vi\'egas, O.&nbsp;Vinyals, P.&nbsp;Warden,
  M.&nbsp;Wattenberg, M.&nbsp;Wicke, Y.&nbsp;Yu, and X.&nbsp;Zheng.
<br> Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
<br>  CoRR, abs/1603.04467, 2016.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_103"><b>[103]</b></td>  <td valign="top">G.&nbsp;Fursin, J.&nbsp;Cavazos, M.&nbsp;O'Boyle, and O.&nbsp;Temam.
<br> MiDataSets: Creating the conditions for a more realistic evaluation
  of iterative optimization.
<br> In  Proceedings of the International Conference on High
  Performance Embedded Architectures & Compilers (HiPEAC 2007), January 2007.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_104"><b>[104]</b></td>  <td valign="top">G.&nbsp;Fursin.
<br>  Iterative Compilation and Performance Prediction for Numerical
  Applications.
<br> PhD thesis, University of Edinburgh, United Kingdom, 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_105"><b>[105]</b></td>  <td valign="top">M.&nbsp;P\"uschel, J.&nbsp;M.&nbsp;F. Moura, B.&nbsp;Singer, J.&nbsp;Xiong, J.&nbsp;R. Johnson, D.&nbsp;A.
  Padua, M.&nbsp;M. Veloso, and R.&nbsp;W. Johnson.
<br> Spiral: A generator for platform-adapted libraries of signal
  processing alogorithms.
<br>  IJHPCA, 18(1):21--45, 2004.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_106"><b>[106]</b></td>  <td valign="top">Y.&nbsp;Liu, E.&nbsp;Z. Zhang, and X.&nbsp;Shen.
<br> A cross-input adaptive framework for gpu program optimizations.
<br> In  2009 IEEE International Symposium on Parallel Distributed
  Processing, pages 1--10, May 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_107"><b>[107]</b></td>  <td valign="top">L.&nbsp;Luo, Y.&nbsp;Chen, C.&nbsp;Wu, S.&nbsp;Long, and G.&nbsp;Fursin.
<br> Finding representative sets of optimizations for adaptive
  multiversioning applications.
<br> In  3rd Workshop on Statistical and Machine Learning Approaches
  Applied to Architectures and Compilation (SMART'09), colocated with HiPEAC'09
  conference, January 2009.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_108"><b>[108]</b></td>  <td valign="top">Kaggle: platform for predictive modelling and analytics competitions.
<br> <a href="https://www.kaggle.com">Link</a>, 2010.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_109"><b>[109]</b></td>  <td valign="top">Imagenet challenge (ILSVRC): Imagenet large scale visual recognition
  challenge where software programs compete to correctly classify and detect
  objects and scenes.
<br> <a href="http://www.image-net.org">Link</a>, 2010.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_110"><b>[110]</b></td>  <td valign="top">LPIRC: low-power image recognition challenge.
<br> <a href="https://rebootingcomputing.ieee.org/lpirc">Link</a>, 2015.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_111"><b>[111]</b></td>  <td valign="top">Public repositories with artifact and workflows in the Collective Knowledge
  format.
<br> <a href="http://cKnowledge.org/shared-repos">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_112"><b>[112]</b></td>  <td valign="top">Association for Computing Machinery (ACM).
<br> <a href="http://www.acm.org">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_113"><b>[113]</b></td>  <td valign="top">P.&nbsp;Flick, C.&nbsp;Jain, T.&nbsp;Pan, and S.&nbsp;Aluru.
<br> A parallel connectivity algorithm for de bruijn graphs in metagenomic
  applications.
<br> In  Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis, SC '15, pages
  15:1--15:11, New York, NY, USA, 2015. ACM.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_114"><b>[114]</b></td>  <td valign="top">L.&nbsp;Nardi, B.&nbsp;Bodin, M.&nbsp;Z. Zia, J.&nbsp;Mawer, A.&nbsp;Nisbet, P.&nbsp;H.&nbsp;J. Kelly, A.&nbsp;J.
  Davison, M.&nbsp;Luj\'an, M.&nbsp;F.&nbsp;P. O'Boyle, G.&nbsp;Riley, N.&nbsp;Topham, and S.&nbsp;Furber.
<br> Introducing SLAMBench, a performance and accuracy benchmarking
  methodology for SLAM.
<br> In  IEEE Intl. Conf. on Robotics and Automation (ICRA), May
  2015.
<br> arXiv:1410.2167.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_115"><b>[115]</b></td>  <td valign="top">Collective Knowledge workflows for SLAMBench.
<br> <a href="https://github.com/ctuning/reproduce-pamela-project">Link</a>.  <br></td>
 </tr>
 <tr>
  <td valign="top"><a name="ref_116"><b>[116]</b></td>  <td valign="top">G.&nbsp;Fursin, A.&nbsp;Lokhmotov, and E.&nbsp;Upton.
<br> Collective knowledge repository with reproducible experimental
  results from collaborative program autotuning on raspberry pi (program
  reactions to most efficient compiler optimizations).
<br> https://doi.org/10.6084/m9.figshare.5789007.v2, Jan 2018.  <br></td>
 </tr>
</table>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>